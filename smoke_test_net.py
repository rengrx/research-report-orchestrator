import os
import json
import hashlib
import time
import requests
import logging

# --- æ¨¡æ‹Ÿé…ç½® (è¯·ç¡®ä¿ä¸ä½ çœŸå®ç¯å¢ƒä¸€è‡´) ---
# 1. è·å– API Key
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# 2. é…ç½®ä»£ç† (å¦‚æœæœ‰)
# å¦‚æœä½ éœ€è¦ä»£ç†ï¼Œè¯·å–æ¶ˆæ³¨é‡Šå¹¶å¡«å…¥ï¼Œå¦åˆ™ä¿æŒä¸º None (æ¨¡æ‹Ÿ Config.PROXIES_CLOUD)
PROXIES = None 
# PROXIES = {"http": "http://127.0.0.1:7890", "https": "http://127.0.0.1:7890"} 

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def test_search_robust(query):
    logging.info(f"ğŸ§ª å¼€å§‹ç½‘ç»œå†’çƒŸæµ‹è¯•: Query='{query}'")
    
    # 1. æ£€æŸ¥ Key
    if not TAVILY_API_KEY:
        logging.error("âŒ æœªæ£€æµ‹åˆ° TAVILY_API_KEYï¼è¯·å…ˆ export TAVILY_API_KEY=Tvly-xxxx")
        return

    # 2. è®¡ç®— MD5 (éªŒè¯å“ˆå¸Œé€»è¾‘)
    query_hash = hashlib.md5(query.encode("utf-8")).hexdigest()
    cache_dir = os.path.expanduser("~/mineru/workflow/search_cache")
    os.makedirs(cache_dir, exist_ok=True)
    cache_path = os.path.join(cache_dir, f"{query_hash}.json")
    logging.info(f"ğŸ“‚ é¢„æœŸç¼“å­˜è·¯å¾„: {cache_path}")

    # 3. æ¸…ç†æ—§ç¼“å­˜ (ä¸ºäº†æµ‹è¯•çœŸå®ç½‘ç»œè¯·æ±‚)
    if os.path.exists(cache_path):
        os.remove(cache_path)
        logging.info("ğŸ§¹ å·²æ¸…é™¤æ—§ç¼“å­˜ï¼Œå¼ºåˆ¶å‘èµ·ç½‘ç»œè¯·æ±‚...")

    # 4. å‘èµ·è¯·æ±‚ (éªŒè¯ä»£ç†å’Œ Tavily è¿é€šæ€§)
    url = "https://api.tavily.com/search"
    payload = {
        "api_key": TAVILY_API_KEY,
        "query": query,
        "search_depth": "advanced",
        "include_answer": True,
        "max_results": 2
    }

    try:
        start_time = time.time()
        logging.info("ğŸŒ å‘é€è¯·æ±‚åˆ° Tavily...")
        
        # æ¨¡æ‹Ÿ retry é€»è¾‘ä¸­çš„ä¸€æ¬¡è¯·æ±‚
        response = requests.post(
            url, 
            json=payload, 
            headers={"Content-Type": "application/json"},
            proxies=PROXIES, # æµ‹è¯•ä»£ç†
            timeout=15
        )
        
        if response.status_code == 200:
            duration = time.time() - start_time
            data = response.json()
            answer = data.get("answer", "æ— æ‘˜è¦")
            logging.info(f"âœ… ç½‘ç»œè¯·æ±‚æˆåŠŸ! (è€—æ—¶: {duration:.2f}s)")
            logging.info(f"ğŸ“„ è¿”å›æ‘˜è¦: {answer[:50]}...")
            
            # 5. å†™å…¥ç¼“å­˜ (éªŒè¯å†™æƒé™)
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False)
            logging.info("ğŸ’¾ ç¼“å­˜å†™å…¥æˆåŠŸ")
            
            # 6. äºŒæ¬¡è¯»å– (éªŒè¯ç¼“å­˜å‘½ä¸­é€»è¾‘)
            if os.path.exists(cache_path):
                logging.info("ğŸ” éªŒè¯ç¼“å­˜æ–‡ä»¶å­˜åœ¨: æ˜¯")
            else:
                logging.error("âŒ ç¼“å­˜æ–‡ä»¶æœªç”Ÿæˆï¼")
                
        elif response.status_code == 403:
            logging.error("âŒ API Key æ— æ•ˆ (403 Forbidden)")
        else:
            logging.error(f"âŒ è¯·æ±‚å¤±è´¥: Status {response.status_code} - {response.text}")

    except Exception as e:
        logging.error(f"âŒ ç½‘ç»œè¿æ¥å¼‚å¸¸ (å¯èƒ½æ˜¯ä»£ç†é…ç½®é”™è¯¯): {e}")

if __name__ == "__main__":
    # æµ‹è¯•ä¸€ä¸ªæ°¸è¿œä¸ä¼šå˜çš„çƒ­ç‚¹è¯ï¼Œæˆ–è€…å¸¦æ—¶é—´æˆ³çš„è¯ä»¥ç¡®ä¿ç»“æœæ–°é²œ
    test_search_robust("DeepSeek-R1 technical report analysis")