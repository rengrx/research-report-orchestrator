# ğŸš€ æœç´¢èƒ½åŠ›æ”¹è¿›å®ç°æ¸…å•

## å¿«é€Ÿæ‘˜è¦

å½“å‰è„šæœ¬æœç´¢è¯„åˆ†ï¼š**6.8/10** â­
- âœ… å®¹é”™èƒ½åŠ›å¼º
- âš ï¸ æœç´¢ç²¾åº¦ä¸­ç­‰
- âŒ ç¼ºä¹è¯­ä¹‰ç†è§£

---

## ğŸ“‹ æ”¹è¿›é¡¹ç›®åˆ—è¡¨

### Tier 1ï¼šå¿«é€Ÿæ”¶ç›Šï¼ˆ1-2 å°æ—¶ï¼‰

#### 1.1 ä½¿ç”¨ BM25 æ›¿ä»£ TF-IDF
**æ–‡ä»¶ä½ç½®ï¼š** `_build_vector_index()` ç¬¬ 545 è¡Œ
**æ”¹è¿›å‰ï¼š**
```python
self.vectorizer = TfidfVectorizer(max_features=50000)
```
**æ”¹è¿›åï¼š**
```python
# éœ€è¦å®‰è£…: pip install rank-bm25
from rank_bm25 import BM25Okapi
corpus = [c.get("text", "") for c in valid_chunks]
self.bm25_model = BM25Okapi([list(tokenize(doc)) for doc in corpus])
```
**é¢„æœŸæ•ˆæœï¼š** ç²¾åº¦ +20%
**æˆæœ¬ï¼š** ä½
**ä¼˜å…ˆçº§ï¼š** ğŸ”´ æœ€é«˜

---

#### 1.2 ä¸­æ–‡åˆ†è¯ä¼˜åŒ–
**æ–‡ä»¶ä½ç½®ï¼š** `retrieve()` ç¬¬ 610 è¡Œ
**æ”¹è¿›å‰ï¼š**
```python
query_words = set(re.findall(r"[\u4e00-\u9fff]+|[a-zA-Z0-9]+", query))
```
**æ”¹è¿›åï¼š**
```python
# éœ€è¦å®‰è£…: pip install jieba
import jieba
# ä¿ç•™è‹±æ–‡å’Œæ•°å­—
query_words = set(jieba.cut(query)) | set(re.findall(r"[a-zA-Z0-9]+", query))
```
**é¢„æœŸæ•ˆæœï¼š** å¬å›ç‡ +30%
**æˆæœ¬ï¼š** ä½
**ä¼˜å…ˆçº§ï¼š** ğŸ”´ æœ€é«˜

---

#### 1.3 æŸ¥è¯¢å…³é”®è¯æ£€æŸ¥æ”¹è¿›
**æ–‡ä»¶ä½ç½®ï¼š** `search_web()` ç¬¬ 715 è¡Œ
**æ”¹è¿›å‰ï¼š**
```python
urgent_keywords = ['æ•°æ®', 'ç»Ÿè®¡', 'æœ€æ–°', '2024', '2025', 'æŠ¥å‘Š', 'æŒ‡æ•°', 'æ’å', 'åˆ†æ', 'ç°çŠ¶']
if not any(kw in query for kw in urgent_keywords):
    return ""
```
**æ”¹è¿›åï¼š**
```python
# é…ç½®åŒ–å…³é”®è¯ï¼Œæ”¯æŒåŠ¨æ€è°ƒæ•´
urgent_keywords = getattr(CONF, "SEARCH_KEYWORDS", 
    ['æ•°æ®', 'ç»Ÿè®¡', 'æœ€æ–°', 'æŠ¥å‘Š', 'æŒ‡æ•°', 'æ’å', 'åˆ†æ', 'ç°çŠ¶'])
# å…è®¸æŸ¥è¯¢ä¸ºç©ºä½† force=True
if query and (force or any(kw in query for kw in urgent_keywords)):
    # æ‰§è¡Œæœç´¢
```
**é¢„æœŸæ•ˆæœï¼š** ç”¨æˆ·ä½“éªŒ +15%
**æˆæœ¬ï¼š** æä½
**ä¼˜å…ˆçº§ï¼š** ğŸŸ¡ ä¸­

---

#### 1.4 åŠ¨æ€ top_k è°ƒæ•´
**æ–‡ä»¶ä½ç½®ï¼š** `retrieve()` ç¬¬ 660 è¡Œ
**æ”¹è¿›å‰ï¼š**
```python
context = ""
for rank, (s, idx) in enumerate(combined[:top_k], start=1):
```
**æ”¹è¿›åï¼š**
```python
# æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦åŠ¨æ€è°ƒæ•´
query_complexity = len(query_words)
dynamic_top_k = min(
    max(top_k, query_complexity * 2 + 2),  # æœ€å°‘ä¸º top_k
    len(combined)  # ä¸è¶…è¿‡æ€»æ•°
)
context = ""
for rank, (s, idx) in enumerate(combined[:dynamic_top_k], start=1):
```
**é¢„æœŸæ•ˆæœï¼š** ç›¸å…³æ€§ +10%
**æˆæœ¬ï¼š** æä½
**ä¼˜å…ˆçº§ï¼š** ğŸŸ¡ ä¸­

---

### Tier 2ï¼šä¸­æœŸæ”¹è¿›ï¼ˆ2-4 å°æ—¶ï¼‰

#### 2.1 æŸ¥è¯¢æ‰©å±•ï¼ˆQuery Expansionï¼‰
**æ–‡ä»¶ä½ç½®ï¼š** éœ€è¦æ–°å¢å‡½æ•°
**ç¤ºä¾‹å®ç°ï¼š**
```python
def expand_query(query, synonyms_dict=None):
    """æ‰©å±•æŸ¥è¯¢ä»¥æé«˜å¬å›ç‡"""
    # åŒä¹‰è¯æ‰©å±•
    synonyms_dict = synonyms_dict or {
        "ç”µåŠ›": ["ç”µèƒ½", "ç”µåŠ›"],
        "äº¤æ˜“": ["äº¤æ˜“", "è´¸æ˜“", "äº¤æ˜“å¸‚åœº"],
        "ç°è´§": ["ç°è´§", "å³æœŸ"],
    }
    
    expanded = [query]
    for word, variants in synonyms_dict.items():
        if word in query:
            for variant in variants:
                expanded.append(query.replace(word, variant))
    
    return expanded
```
**é¢„æœŸæ•ˆæœï¼š** å¬å›ç‡ +30%
**æˆæœ¬ï¼š** ä¸­
**ä¼˜å…ˆçº§ï¼š** ğŸŸ¡ ä¸­

---

#### 2.2 æœç´¢ç»“æœæ’åºä¼˜åŒ–
**æ–‡ä»¶ä½ç½®ï¼š** `retrieve()` ç¬¬ 650 è¡Œ
**æ”¹è¿›ï¼š** å¤šä¿¡å·æ’åº
```python
def compute_relevance_score(chunk, query_words, tfidf_score, keyword_score):
    """ç»¼åˆè€ƒè™‘å¤šä¸ªä¿¡å·è®¡ç®—ç›¸å…³æ€§"""
    # ä¿¡å· 1: TF-IDF ç›¸ä¼¼åº¦ï¼ˆæƒé‡ 40%ï¼‰
    # ä¿¡å· 2: å…³é”®è¯åŒ¹é…ï¼ˆæƒé‡ 30%ï¼‰
    # ä¿¡å· 3: å—å¤§å°ï¼ˆæƒé‡ 20%ï¼‰- æ›´é•¿çš„æ–‡æœ¬é€šå¸¸æ›´æœ‰ä»·å€¼
    # ä¿¡å· 4: æ¥æºæƒé‡ï¼ˆæƒé‡ 10%ï¼‰
    
    size_score = min(len(chunk['text']) / 500, 1.0)  # å½’ä¸€åŒ–
    source_weight = chunk.get('weight', 1.0)
    
    total = (tfidf_score * 0.4 + 
             keyword_score * 0.3 + 
             size_score * 0.2 + 
             source_weight * 0.1)
    
    return total
```
**é¢„æœŸæ•ˆæœï¼š** ç²¾åº¦ +15%
**æˆæœ¬ï¼š** ä¸­
**ä¼˜å…ˆçº§ï¼š** ğŸŸ¡ ä¸­

---

#### 2.3 ç¼“å­˜ç®¡ç†ä¼˜åŒ–
**æ–‡ä»¶ä½ç½®ï¼š** `search_web()` ç¬¬ 700 è¡Œ
**æ”¹è¿›ï¼š**
```python
class SearchCache:
    """æ”¹è¿›çš„æœç´¢ç¼“å­˜ç®¡ç†"""
    
    def __init__(self, cache_dir, ttl_hours=24):
        self.cache_dir = cache_dir
        self.ttl_seconds = ttl_hours * 3600
    
    def get(self, query):
        """è·å–ç¼“å­˜ï¼Œè‡ªåŠ¨è¿‡æœŸæ£€æŸ¥"""
        cache_file = self._get_cache_file(query)
        if os.path.exists(cache_file):
            try:
                with open(cache_file) as f:
                    data = json.load(f)
                if time.time() - data['timestamp'] < self.ttl_seconds:
                    return data['result']
                else:
                    os.remove(cache_file)  # åˆ é™¤è¿‡æœŸç¼“å­˜
            except:
                pass
        return None
    
    def set(self, query, result):
        """ä¿å­˜ç¼“å­˜"""
        cache_file = self._get_cache_file(query)
        with open(cache_file, 'w') as f:
            json.dump({
                'query': query,
                'result': result,
                'timestamp': time.time()
            }, f, ensure_ascii=False)
    
    def clear(self, older_than_hours=None):
        """æ¸…é™¤æ‰€æœ‰æˆ–è¿‡æœŸç¼“å­˜"""
        for file in os.listdir(self.cache_dir):
            if file.endswith('.json'):
                path = os.path.join(self.cache_dir, file)
                if older_than_hours:
                    age_hours = (time.time() - os.path.getmtime(path)) / 3600
                    if age_hours > older_than_hours:
                        os.remove(path)
                else:
                    os.remove(path)
```
**é¢„æœŸæ•ˆæœï¼š** ç”¨æˆ·ä½“éªŒ +20%
**æˆæœ¬ï¼š** ä¸­
**ä¼˜å…ˆçº§ï¼š** ğŸŸ¡ ä¸­

---

### Tier 3ï¼šé•¿æœŸæ¶æ„æ”¹è¿›ï¼ˆ1-2 å¤©ï¼‰

#### 3.1 é›†æˆå‘é‡æ•°æ®åº“
**æ–¹æ¡ˆï¼š** ä½¿ç”¨ Milvus / Weaviate / Pinecone
```python
# ç¤ºä¾‹ï¼šä½¿ç”¨ chromadbï¼ˆè½»é‡çº§ï¼‰
from chromadb import Client
from chromadb.config import Settings

client = Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_db",
))

collection = client.get_or_create_collection(name="research_materials")

# ç´¢å¼•æ–‡æ¡£
for chunk in valid_chunks:
    collection.add(
        ids=[str(idx)],
        documents=[chunk['text']],
        metadatas=[chunk['metadata']],
    )

# æœç´¢
results = collection.query(
    query_texts=[query],
    n_results=top_k,
)
```
**é¢„æœŸæ•ˆæœï¼š** ç²¾åº¦ +40-50%
**æˆæœ¬ï¼š** é«˜ï¼ˆæ¶æ„é‡æ„ï¼‰
**ä¼˜å…ˆçº§ï¼š** ğŸŸ¢ ä½ï¼ˆä½†é•¿æœŸå›æŠ¥é«˜ï¼‰

---

#### 3.2 é›†æˆè¯­ä¹‰æœç´¢ï¼ˆEmbeddingï¼‰
**æ–¹æ¡ˆï¼š** ä½¿ç”¨é¢„è®­ç»ƒ embedding æ¨¡å‹
```python
# ç¤ºä¾‹ï¼šä½¿ç”¨å¼€æºä¸­æ–‡ embedding æ¨¡å‹
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

# ç¦»çº¿ç¼–ç æ‰€æœ‰ chunks
embeddings = model.encode([c['text'] for c in valid_chunks])

# åœ¨çº¿æœç´¢
query_embedding = model.encode(query)
similarities = cosine_similarity([query_embedding], embeddings)[0]
top_indices = np.argsort(similarities)[::-1][:top_k]
```
**é¢„æœŸæ•ˆæœï¼š** ç²¾åº¦ +50-60%
**æˆæœ¬ï¼š** é«˜ï¼ˆæ¨¡å‹ç»´æŠ¤ï¼‰
**ä¼˜å…ˆçº§ï¼š** ğŸŸ¢ ä½ï¼ˆä½†å…³é”®ä»·å€¼é«˜ï¼‰

---

#### 3.3 æœç´¢æ•ˆæœè¯„ä¼°ä½“ç³»
**æ–¹æ¡ˆï¼š** å»ºç«‹æµ‹è¯•é›†å’Œåº¦é‡
```python
class SearchEvaluator:
    """æœç´¢æ•ˆæœè¯„ä¼°"""
    
    def __init__(self):
        # æµ‹è¯•é›†: (æŸ¥è¯¢, ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨)
        self.test_queries = [
            ("ç”µåŠ›å¸‚åœºäº¤æ˜“", [doc_id_1, doc_id_2, ...]),
            ("ç°è´§ä»·æ ¼", [doc_id_3, doc_id_4, ...]),
            # ...
        ]
    
    def evaluate(self, search_func):
        """è¯„ä¼°æœç´¢å‡½æ•°"""
        metrics = {
            'precision@5': [],
            'recall@5': [],
            'mrr': [],  # Mean Reciprocal Rank
        }
        
        for query, relevant_docs in self.test_queries:
            results = search_func(query, top_k=5)
            result_ids = [r['id'] for r in results]
            
            # è®¡ç®—æŒ‡æ ‡
            precision = len(set(result_ids) & set(relevant_docs)) / 5
            recall = len(set(result_ids) & set(relevant_docs)) / len(relevant_docs)
            
            metrics['precision@5'].append(precision)
            metrics['recall@5'].append(recall)
        
        return {k: np.mean(v) for k, v in metrics.items()}
```
**é¢„æœŸæ•ˆæœï¼š** å¯åº¦é‡åŒ–çš„æ”¹è¿›
**æˆæœ¬ï¼š** é«˜ï¼ˆéœ€è¦æ ‡æ³¨æ•°æ®ï¼‰
**ä¼˜å…ˆçº§ï¼š** ğŸŸ¢ ä½ï¼ˆä½†æ–¹å‘æ€§é‡è¦ï¼‰

---

## ğŸ¯ å®æ–½è·¯çº¿å›¾

### ç¬¬ä¸€å‘¨ï¼šå¿«é€Ÿèµ¢
```
Day 1-2: 
  â˜ é›†æˆ BM25
  â˜ é›†æˆ jieba åˆ†è¯
  â˜ ä¿®å¤æŸ¥è¯¢å…³é”®è¯æ£€æŸ¥

Day 3:
  â˜ åŠ¨æ€ top_k è°ƒæ•´
  â˜ æµ‹è¯•å’ŒéªŒè¯
  â˜ æ€§èƒ½åŸºå‡†æµ‹è¯•

é¢„æœŸæ”¶ç›Šï¼šç²¾åº¦ +25%ï¼Œå¬å›ç‡ +30%
```

### ç¬¬äºŒå‘¨ï¼šä¸­æœŸä¼˜åŒ–
```
Day 4-5:
  â˜ å®ç°æŸ¥è¯¢æ‰©å±•
  â˜ å¤šä¿¡å·æ’åº

Day 6-7:
  â˜ ç¼“å­˜ç®¡ç†ä¼˜åŒ–
  â˜ æœç´¢æ—¥å¿—è®°å½•
  â˜ é›†æˆæµ‹è¯•

é¢„æœŸæ”¶ç›Šï¼šç”¨æˆ·ä½“éªŒ +20%ï¼Œæ€§èƒ½ +15%
```

### ç¬¬ä¸‰å‘¨åŠä»¥åï¼šæ¶æ„å‡çº§
```
Week 3-4:
  â˜ è¯„ä¼°å‘é‡æ•°æ®åº“æ–¹æ¡ˆ
  â˜ POC å®ç°
  â˜ æ€§èƒ½å¯¹æ¯”æµ‹è¯•

Week 5-6:
  â˜ é›†æˆè¯­ä¹‰æœç´¢
  â˜ å»ºç«‹è¯„ä¼°ä½“ç³»
  â˜ æŒç»­ä¼˜åŒ–

é¢„æœŸæ”¶ç›Šï¼šç²¾åº¦ +50%ï¼Œç”¨æˆ·æ»¡æ„åº¦å¤§å¹…æå‡
```

---

## ğŸ“¦ æ‰€éœ€ä¾èµ–å®‰è£…

### å¿«é€Ÿç‰ˆ
```bash
pip install rank-bm25 jieba
```

### å®Œæ•´ç‰ˆ
```bash
pip install rank-bm25 jieba sentence-transformers chromadb milvus-lite
```

### å¯é€‰é«˜çº§ç‰ˆ
```bash
pip install elasticsearch opensearch-py pinecone weaviate-client
```

---

## ğŸ’¾ æ€§èƒ½é¢„æœŸ

| æ–¹æ¡ˆ | ç²¾åº¦ | é€Ÿåº¦ | æˆæœ¬ | éš¾åº¦ |
|------|------|------|------|------|
| å½“å‰ | 5/10 | å¿« | ä½ | ç®€ |
| Tier 1 | 7/10 | å¿« | ä½ | ç®€ |
| Tier 2 | 8/10 | ä¸­ | ä¸­ | ä¸­ |
| Tier 3 | 9/10 | ä¸­ | é«˜ | é«˜ |

---

## âœ… æˆåŠŸæŒ‡æ ‡

- âœ… ç²¾åº¦æå‡è‡³ 7.5+/10
- âœ… å¹³å‡æŸ¥è¯¢æ—¶é—´ < 2s
- âœ… ç¼“å­˜å‘½ä¸­ç‡ > 70%
- âœ… ç”¨æˆ·æ»¡æ„åº¦åé¦ˆ > 80%

---

**åˆ›å»ºæ—¥æœŸï¼š** 2025å¹´11æœˆ29æ—¥
**ç‰ˆæœ¬ï¼š** V1.0
