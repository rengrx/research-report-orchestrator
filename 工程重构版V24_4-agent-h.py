import argparse
import subprocess
import time
import requests
import re
import json
import os
import sys
import random
import shutil
import glob
import traceback
import hashlib
import base64
from io import BytesIO
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯ï¼Œé¿å…æ˜¾ç¤ºçª—å£
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from datetime import datetime

# ä¼˜å…ˆå°è¯•ä½¿ç”¨ LangChain çš„æ ‡é¢˜/é€’å½’åˆ‡åˆ†ï¼Œå¦‚ç¼ºå¤±åˆ™é™çº§
try:
    from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
    from langchain.schema import Document
    HAS_LANGCHAIN = True
except ImportError:
    try:
        from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
        from langchain.schema import Document
        HAS_LANGCHAIN = True
    except ImportError:
        HAS_LANGCHAIN = False
        print("âš ï¸ æœªæ£€æµ‹åˆ° langchain/text-splittersï¼Œåˆ†å—å°†å›é€€ä¸ºç®€æ˜“å­—ç¬¦åˆ‡åˆ†ã€‚å»ºè®®: pip install langchain langchain-text-splitters")

# ================== ğŸ¨ å­—ä½“ç®¡ç† (è§£å†³ä¸­æ–‡ä¹±ç ) ==================

def get_chinese_font():
    """åŠ¨æ€æŸ¥æ‰¾ç³»ç»Ÿä¸­å¯ç”¨çš„ä¸­æ–‡å­—ä½“"""
    # å¸¸è§çš„ä¸­æ–‡å­—ä½“è·¯å¾„ (macOS, Linux, Windows)
    font_candidates = [
        # macOS
        "/System/Library/Fonts/PingFang.ttc",
        "/System/Library/Fonts/STHeiti Light.ttc",
        "/System/Library/Fonts/STHeiti Medium.ttc",
        "/Library/Fonts/Arial Unicode.ttf",
        "/System/Library/Fonts/Supplemental/Arial Unicode.ttf",
        # Linux
        "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",
        "/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf",
        # Windows
        "C:\\Windows\\Fonts\\simhei.ttf",
        "C:\\Windows\\Fonts\\msyh.ttc",
    ]
    
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for path in font_candidates:
        if os.path.exists(path):
            return fm.FontProperties(fname=path)
            
    # 2. å¦‚æœæ²¡æ‰¾åˆ°æ–‡ä»¶ï¼Œå°è¯•é€šè¿‡ family name è·å– (Matplotlib é»˜è®¤æœºåˆ¶)
    common_families = ['SimHei', 'Arial Unicode MS', 'PingFang SC', 'Heiti TC', 'Microsoft YaHei', 'WenQuanYi Micro Hei']
    for family in common_families:
        try:
            # å°è¯•åŠ è½½å­—ä½“
            fm.findfont(family, fallback_to_default=False)
            return fm.FontProperties(family=family)
        except:
            continue
            
    return None

# åˆå§‹åŒ–å­—ä½“
CHINESE_FONT = get_chinese_font()
if CHINESE_FONT:
    # å…¨å±€è®¾ç½®
    try:
        font_name = CHINESE_FONT.get_name() if hasattr(CHINESE_FONT, 'get_name') else 'Unknown'
        plt.rcParams['font.family'] = font_name
    except:
        # å¦‚æœæ˜¯è·¯å¾„åˆ›å»ºçš„ FontPropertiesï¼Œget_name å¯èƒ½è¿”å›æ–‡ä»¶åï¼Œè¿™é‡Œåšå®¹é”™
        font_name = 'Loaded'
    print(f"âœ… å·²åŠ è½½ä¸­æ–‡å­—ä½“: {font_name}")
else:
    print("âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¹±ç ")

plt.rcParams['axes.unicode_minus'] = False  # å¤„ç†è´Ÿå·æ˜¾ç¤º

# ================== âš™ï¸ é…ç½®ç®¡ç†ç±» (è§£å†³ç¡¬ç¼–ç é—®é¢˜) ==================

class Config:
    def __init__(self):
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦åˆ™è®¾ä¸º None
        self.GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        self.TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "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")
        
        # é»˜è®¤é…ç½®
        self.GEMINI_OUTLINE_MODEL = "gemini-3-pro-preview"  # å‡çº§ï¼šä½¿ç”¨æœ€æ–° Gemini 3.0 Pro æ„å»ºå¤§çº²
        self.GEMINI_GEN_MODEL = "gemini-3-pro-preview"     # å‡çº§ï¼šä½¿ç”¨æœ€æ–° Gemini 3.0 Pro ç¡®ä¿æœ€ä½³æ€§èƒ½
        self.GEMINI_PRO_MODEL = os.getenv("GEMINI_PRO_MODEL", "gemini-3-pro-preview")   # Pro è§’è‰²ï¼ˆè§„åˆ’/æ¶¦è‰²/ä»£ç ï¼‰
        self.GEMINI_FLASH_MODEL = os.getenv("GEMINI_FLASH_MODEL", "gemini-2.5-flash")   # Flash è§’è‰²ï¼ˆæœç´¢/ç²—å†™/æ•°æ®æ•´ç†ï¼‰
        self.GEMINI_FLASH_LITE_MODEL = os.getenv("GEMINI_FLASH_LITE_MODEL", "gemini-2.5-flash-lite")  # æè‡´çœé’±ç‰ˆ
        
        # è·¯å¾„é…ç½® (ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ç”¨æˆ·ç›®å½•ï¼Œé¿å…ç¡¬ç¼–ç ç»å¯¹è·¯å¾„)
        self.BASE_DIR = os.path.join(os.path.expanduser("~"), "Research_Workspace")
        
        # å¤§çº²ç»“æ„é…ç½® (å¯åŠ¨æ€è°ƒæ•´)
        self.OUTLINE_CHAPTERS = int(os.getenv("OUTLINE_CHAPTERS", 8))  # ç« èŠ‚æ•°
        self.OUTLINE_SECTIONS = int(os.getenv("OUTLINE_SECTIONS", 3))  # æ¯ç« å°èŠ‚æ•°
        self.OUTLINE_SUBSECTIONS = int(os.getenv("OUTLINE_SUBSECTIONS", 3))  # æ¯å°èŠ‚å­èŠ‚æ•°
        
        # ç›®æ ‡é¡µæ•°é…ç½® (è‡ªåŠ¨è®¡ç®—ç« èŠ‚ç»“æ„)
        self.TARGET_PAGES = int(os.getenv("TARGET_PAGES", 0))  # 0 è¡¨ç¤ºä¸è‡ªåŠ¨è°ƒæ•´
        self.WORDS_PER_PAGE = int(os.getenv("WORDS_PER_PAGE", 500))  # æ¯é¡µå¹³å‡å­—æ•°ï¼ˆå«æ ¼å¼ï¼‰
        
        # å†™ä½œæƒé‡é…ç½®
        self.SECTION_WEIGHT = float(os.getenv("SECTION_WEIGHT", 0.5))  # å°èŠ‚æƒé‡ï¼ˆvs å­èŠ‚ï¼‰
        self.QUALITY_THRESHOLD = float(os.getenv("QUALITY_THRESHOLD", 8.0))  # æé«˜ä¸º8.0ï¼Œç¡®ä¿é«˜è´¨é‡
        self.MAX_REFINEMENT_ROUNDS = int(os.getenv("MAX_REFINEMENT_ROUNDS", 4))  # å¢åŠ åˆ°4è½®æ”¹è¿›
        self.USE_TFIDF_RAG = os.getenv("USE_TFIDF_RAG", "true").lower() == "true"  # å¯ç”¨å‘é‡æ£€ç´¢

        # ä»£ç†é…ç½®
        self.PROXY_URL = os.getenv("HTTP_PROXY") or "http://127.0.0.1:6152"
        self.PROXIES_CLOUD = {"http": self.PROXY_URL, "https": self.PROXY_URL}
        self.PROXIES_LOCAL = {"http": None, "https": None} # å¼ºåˆ¶ç›´è¿

        # MinerU é…ç½®ï¼ˆç”¨äºæ›´å¿«çš„ç´ æè§£æï¼‰
        self.USE_MINERU = os.getenv("USE_MINERU", "true").lower() == "true"
        default_mineru_home = os.path.expanduser("~/mineru")
        default_mineru_cmds = [
            os.path.join(default_mineru_home, "bin", "mineru"),                   # æœ¬æœºå®‰è£… (macOS M2)
            "/opt/anaconda3/envs/mineru_env/bin/mineru",                         # å¤‡é€‰: anaconda ç¯å¢ƒ
            "/opt/anaconda3/envs/mineru262-env/bin/mineru"                       # æ—§è·¯å¾„å…¼å®¹
        ]
        env_mineru_cmd = os.getenv("MINERU_CMD")
        candidate_cmds = []
        if env_mineru_cmd:
            candidate_cmds.append(os.path.expanduser(env_mineru_cmd))
        candidate_cmds.extend(default_mineru_cmds)
        resolved_cmd = None
        for candidate in candidate_cmds:
            expanded = os.path.expanduser(candidate)
            if os.path.exists(expanded):
                resolved_cmd = expanded
                break
        if not resolved_cmd:
            print("âš ï¸ æœªåœ¨é¢„è®¾è·¯å¾„æ‰¾åˆ° MinerUï¼Œå°†å°è¯•ä½¿ç”¨ç³»ç»Ÿè·¯å¾„ä¸­çš„ 'mineru'")
            resolved_cmd = "mineru"
        self.MINERU_CMD = resolved_cmd
        self.MINERU_IN_DIR = os.path.expanduser(os.getenv("MINERU_IN", os.path.join(default_mineru_home, "workflow", "in")))
        self.MINERU_OUT_DIR = os.path.expanduser(os.getenv("MINERU_OUT", os.path.join(default_mineru_home, "workflow", "out")))
        self.MINERU_METHOD = os.getenv("MINERU_METHOD", "auto")
        self.MINERU_BACKEND = os.getenv("MINERU_BACKEND", "pipeline")
        self.MINERU_LANG = os.getenv("MINERU_LANG", "ch")
        self.MINERU_TIMEOUT = int(os.getenv("MINERU_TIMEOUT", 600))

    def validate(self):
        """å¯åŠ¨å‰è‡ªæ£€"""
        errors = []
        if not self.GEMINI_API_KEY or "Your_Key" in self.GEMINI_API_KEY:
            # äº¤äº’å¼è¡¥æ•‘
            self.GEMINI_API_KEY = input("âš ï¸ æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼Œè¯·è¾“å…¥: ").strip()
            if not self.GEMINI_API_KEY: errors.append("ç¼ºå°‘ GEMINI_API_KEY")
            
        if not self.TAVILY_API_KEY or "Your_Key" in self.TAVILY_API_KEY:
            print("â„¹ï¸ æœªæ£€æµ‹åˆ° TAVILY_API_KEYï¼Œè”ç½‘æœç´¢åŠŸèƒ½å°†å—é™ã€‚")
            
        if errors:
            print("\nâŒ é…ç½®é”™è¯¯ï¼Œæ— æ³•å¯åŠ¨:")
            for e in errors: print(f"  - {e}")
            sys.exit(1)
    
    def calculate_outline_structure(self):
        """
        æ ¹æ®ç›®æ ‡é¡µæ•°è‡ªåŠ¨è®¡ç®—å¤§çº²ç»“æ„
        è¿”å› (chapters, sections, subsections)
        """
        if self.TARGET_PAGES <= 0:
            # ä¸è‡ªåŠ¨è°ƒæ•´ï¼Œä½¿ç”¨æ‰‹åŠ¨é…ç½®
            return self.OUTLINE_CHAPTERS, self.OUTLINE_SECTIONS, self.OUTLINE_SUBSECTIONS
        
        # åæ¨éœ€è¦çš„æ€»å­—æ•°
        total_words_needed = self.TARGET_PAGES * self.WORDS_PER_PAGE
        
        # æ¯ä¸ªå­èŠ‚å¹³å‡å­—æ•°
        words_per_subsection = 1000
        
        # éœ€è¦çš„å­èŠ‚æ€»æ•°
        total_subsections_needed = total_words_needed // words_per_subsection
        
        # æ ¹æ®å­èŠ‚æ•°åæ¨ç« èŠ‚ç»“æ„
        # ç­–ç•¥ï¼šä¼˜å…ˆå¢åŠ ç« èŠ‚æ•°ï¼Œå…¶æ¬¡å¢åŠ å°èŠ‚æ•°ï¼Œæœ€åå¢åŠ å­èŠ‚æ•°
        
        # å‡è®¾åŸºç¡€é…ç½®
        chapters = max(5, total_subsections_needed // 12)  # è‡³å°‘ 5 ç« 
        sections = 3
        subsections = max(3, total_subsections_needed // (chapters * sections))
        
        # å¾®è°ƒ
        actual_subsections = chapters * sections * subsections
        if actual_subsections < total_subsections_needed:
            subsections += 1
        
        return chapters, sections, subsections
    
    def estimate_page_count(self):
        """ä¼°ç®—æŠ¥å‘Šæ€»é¡µæ•°"""
        total_subsections = self.OUTLINE_CHAPTERS * self.OUTLINE_SECTIONS * self.OUTLINE_SUBSECTIONS
        estimated_words = total_subsections * 1000  # æ¯ä¸ªå­èŠ‚ 1000 å­—
        estimated_pages = estimated_words / self.WORDS_PER_PAGE
        return estimated_pages

# åˆå§‹åŒ–é…ç½®å•ä¾‹
CONF = Config()

# ================== ğŸ“š RAG çŸ¥è¯†åº“ (ä¿®å¤ä¸­æ–‡æ£€ç´¢) ==================

class MaterialManager:
    def __init__(self, folder):
        self.chunks = []
        self.folder = folder
        self.failed_files = []
        self.chunk_stats = {}  # è®°å½•æ¯ä¸ªæ–‡ä»¶çš„ chunk ç»Ÿè®¡
        self.use_mineru = getattr(CONF, "USE_MINERU", False)
        self.mineru_cmd = getattr(CONF, "MINERU_CMD", "")
        self.mineru_in_dir = getattr(CONF, "MINERU_IN_DIR", "")
        self.mineru_out_dir = getattr(CONF, "MINERU_OUT_DIR", "")
        self.mineru_method = getattr(CONF, "MINERU_METHOD", "auto")
        self.mineru_backend = getattr(CONF, "MINERU_BACKEND", "pipeline")
        self.mineru_lang = getattr(CONF, "MINERU_LANG", "ch")
        self.mineru_timeout = getattr(CONF, "MINERU_TIMEOUT", 600)
        self.use_tfidf = getattr(CONF, "USE_TFIDF_RAG", False)
        self.vectorizer = None
        self.tfidf_matrix = None
        self.load()

    def load(self):
        if not os.path.exists(self.folder):
            try:
                os.makedirs(self.folder)
                print(f"ğŸ“‚ [åˆå§‹åŒ–] å·²åˆ›å»ºç´ æç›®å½•: {self.folder}")
                print("ğŸ‘‰ è¯·æ”¾å…¥ PDF/Word/Txt èµ„æ–™ï¼Œè„šæœ¬å°†è‡ªåŠ¨æ‰«æ...")
                # è¿™é‡Œä¸é˜»å¡ï¼Œå…è®¸åç»­æµç¨‹è¿è¡Œ
            except OSError as e:
                print(f"âŒ æ— æ³•åˆ›å»ºç›®å½•: {e}")
                return

        # é€’å½’è·å–æ‰€æœ‰å­ç›®å½•ä¸­çš„æ–‡ä»¶
        files = []
        for root, dirs, filenames in os.walk(self.folder):
            for fname in filenames:
                files.append(os.path.join(root, fname))
        
        print(f"\nğŸ“š æ­£åœ¨æ‰«æç´ æç›®å½•åŠå­ç›®å½•ï¼Œå‘ç° {len(files)} ä»½æ–‡ä»¶...")
        
        loaded_count = 0
        for fpath in files:
            if os.path.basename(fpath).startswith('.'): continue # è·³è¿‡éšè—æ–‡ä»¶
            try:
                ext = os.path.splitext(fpath)[1].lower()
                content = ""
                source_name = os.path.basename(fpath)
                mineru_used = False
                mineru_md_path = None
                source_type = "standard_loader"
                
                mineru_supported = {'.pdf', '.png', '.jpg', '.jpeg', '.bmp', '.webp', '.tiff', '.gif'}
                if self.use_mineru and ext in mineru_supported:
                    mineru_md_path = self._convert_with_mineru(fpath)
                    if mineru_md_path and os.path.exists(mineru_md_path):
                        try:
                            with open(mineru_md_path, 'r', encoding='utf-8', errors='ignore') as f:
                                content = f.read()
                            mineru_used = True
                            source_name = f"{source_name} [MinerU]"
                            source_type = "mineru_markdown"
                        except Exception as e:
                            print(f"âš ï¸ MinerU ç»“æœè¯»å–å¤±è´¥ï¼Œæ”¹ç”¨å†…ç½®è§£æ: {str(e)[:50]}")
                            content = ""
                
                # æ˜¾å¼ä¾èµ–æ£€æŸ¥ä¸é”™è¯¯æ•è·ï¼ˆMinerU å¤±è´¥æ—¶å›é€€ï¼‰
                if not content.strip():
                    if ext == '.pdf':
                        try:
                            import pdfplumber
                            with pdfplumber.open(fpath) as pdf:
                                for p in pdf.pages: content += (p.extract_text() or "") + "\n"
                        except ImportError:
                            self.failed_files.append(f"{os.path.basename(fpath)} (ç¼ºå°‘ pdfplumber åº“)")
                            continue
                            
                    elif ext == '.docx':
                        try:
                            import docx
                            doc = docx.Document(fpath)
                            content = "\n".join([p.text for p in doc.paragraphs])
                        except ImportError:
                            self.failed_files.append(f"{os.path.basename(fpath)} (ç¼ºå°‘ python-docx åº“)")
                            continue
                        except Exception as e:
                            self.failed_files.append(f"{os.path.basename(fpath)} (è¯»å–é”™è¯¯: {str(e)[:50]})")
                            continue
                    
                    elif ext == '.doc':
                        # æ—§æ ¼å¼ .doc æ–‡ä»¶ï¼Œå°è¯•ç”¨ docx åº“å…¼å®¹æ¨¡å¼æˆ–æå–æ–‡æœ¬
                        try:
                            import docx
                            doc = docx.Document(fpath)
                            content = "\n".join([p.text for p in doc.paragraphs])
                        except:
                            # é™çº§å¤„ç†ï¼šç›´æ¥æå–äºŒè¿›åˆ¶æ–‡æœ¬
                            try:
                                with open(fpath, 'rb') as f:
                                    raw = f.read()
                                    # å°è¯•è§£ç ä¸º utf-8 æˆ– gbk
                                    for encoding in ['utf-8', 'gbk', 'latin-1']:
                                        try:
                                            decoded = raw.decode(encoding, errors='ignore')
                                            # æ¸…ç†æ§åˆ¶å­—ç¬¦
                                            content = ''.join(c for c in decoded if ord(c) >= 32 or c in '\n\r\t')
                                            break
                                        except:
                                            continue
                            except:
                                content = ""
                        
                        if not content.strip():
                            self.failed_files.append(f"{os.path.basename(fpath)} (æ— æ³•è¯»å–å†…å®¹)")
                            continue
                            
                    elif ext in ['.txt', '.md']:
                        with open(fpath, 'r', encoding='utf-8', errors='ignore') as f: content = f.read()
                
                if content.strip():
                    # æ™ºèƒ½åˆ†å‰²ï¼šæ ¹æ®å†…å®¹ç»“æ„è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„ chunk (MinerU ä¼˜å…ˆ)
                    file_meta = {"filename": os.path.basename(fpath), "path": fpath}
                    chunks = self.smart_chunk_material(content, source_type, file_meta)
                    
                    for chunk_data in chunks:
                        self.chunks.append(chunk_data)
                    
                    # è®°å½•ç»Ÿè®¡
                    fname = os.path.basename(fpath)
                    self.chunk_stats[fname] = {
                        "chunks": len(chunks),
                        "total_chars": len(content),
                        "status": "æˆåŠŸ",
                        "parser": "MinerU" if mineru_used else "builtin"
                    }
                    loaded_count += 1
                else:
                    self.failed_files.append(f"{os.path.basename(fpath)} (å†…å®¹ä¸ºç©º)")
                    
            except Exception as e:
                self.failed_files.append(f"{os.path.basename(fpath)} (è¯»å–é”™è¯¯: {str(e)})")

        # æ‰“å°åŠ è½½æŠ¥å‘Š
        total_chunks = len(self.chunks)
        print(f"âœ… æˆåŠŸåŠ è½½: {loaded_count} ä»½æ–‡ä»¶ | ç”Ÿæˆ {total_chunks} ä¸ªæ™ºèƒ½ chunk")
        if self.failed_files:
            print("âš ï¸ ä»¥ä¸‹æ–‡ä»¶åŠ è½½å¤±è´¥:")
            for fail in self.failed_files: print(f"  - {fail}")
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        print("\nğŸ“Š ç´ æåŠ è½½ç»Ÿè®¡:")
        for fname, stats in self.chunk_stats.items():
            parser = stats.get("parser", "builtin")
            print(f"  {fname}: {stats['chunks']} chunks ({stats['total_chars']} å­—ç¬¦) [{parser}]")
        
        # æ„å»º TF-IDF å‘é‡ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
        self._build_vector_index()

    def _locate_mineru_md(self, base_name):
        """æŸ¥æ‰¾ MinerU å·²ç”Ÿæˆçš„ Markdownï¼Œé¿å…é‡å¤è§£æ"""
        if not self.mineru_out_dir:
            return None
        candidates = [
            os.path.join(self.mineru_out_dir, f"{base_name}.md"),
            os.path.join(self.mineru_out_dir, base_name, f"{base_name}.md")
        ]
        pattern = os.path.join(self.mineru_out_dir, base_name, "*", f"{base_name}.md")
        candidates.extend(glob.glob(pattern))
        for path in candidates:
            if path and os.path.exists(path):
                return path
        return None

    def _convert_with_mineru(self, fpath):
        """è°ƒç”¨ MinerU å°† PDF/å›¾ç‰‡è§£æä¸º Markdownï¼Œå¤±è´¥åˆ™è¿”å› None"""
        if not self.use_mineru or not self.mineru_cmd or not self.mineru_out_dir:
            return None
        cmd_path = self.mineru_cmd
        if not os.path.exists(cmd_path):
            # å…è®¸èµ° PATH ä¸­çš„ mineru å‘½ä»¤
            from shutil import which
            path_cmd = which(cmd_path)
            if not path_cmd:
                return None
            cmd_path = path_cmd
        base_name = os.path.splitext(os.path.basename(fpath))[0]
        cached_md = self._locate_mineru_md(base_name)
        if cached_md:
            return cached_md
        try:
            os.makedirs(self.mineru_out_dir, exist_ok=True)
            if self.mineru_in_dir:
                try:
                    os.makedirs(self.mineru_in_dir, exist_ok=True)
                    dst = os.path.join(self.mineru_in_dir, os.path.basename(fpath))
                    if not os.path.exists(dst):
                        shutil.copy(fpath, dst)
                except Exception:
                    pass
            cmd = [
                cmd_path,
                "-p", fpath,
                "-o", self.mineru_out_dir,
                "-m", self.mineru_method,
                "-b", self.mineru_backend,
                "-l", self.mineru_lang
            ]
            subprocess.run(cmd, check=True, timeout=self.mineru_timeout)
            return self._locate_mineru_md(base_name)
        except Exception as e:
            print(f"âš ï¸ MinerU è§£æå¤±è´¥: {e}")
            return None
    
    def smart_chunk_material(self, content, source_type, file_meta, chunk_size=800, chunk_overlap=100):
        """
        æ™ºèƒ½åˆ†å—ç­–ç•¥ï¼š
        1) MinerU Markdown: æ ‡é¢˜åˆ‡åˆ† + é€’å½’åˆ‡åˆ†ï¼Œä¿ç•™ H1/H2/H3 å…ƒæ•°æ®
        2) å…¶ä»–/å¤±è´¥: é€’å½’å­—ç¬¦åˆ‡åˆ†ï¼Œä¿ç•™æ–‡ä»¶å…ƒæ•°æ®
        """
        if not content:
            return []

        def _docs_to_chunks(docs, source_label, base_meta):
            chunk_list = []
            for d in docs:
                text = getattr(d, "page_content", None)
                if text is None and isinstance(d, dict):
                    text = d.get("page_content", "")

                meta = {}
                if hasattr(d, "metadata"):
                    meta = dict(getattr(d, "metadata", {}) or {})
                elif isinstance(d, dict):
                    meta = dict(d.get("metadata", {}))
                merged_meta = {**base_meta, **meta, "source_type": source_label}
                chunk_list.append({
                    "source": base_meta.get("filename", "unknown"),
                    "text": text,
                    "size": len(text),
                    "weight": 1.0,
                    "metadata": merged_meta
                })
            return chunk_list

        # LangChain ä¸å¯ç”¨æ—¶ï¼Œç®€æ˜“å­—ç¬¦åˆ‡åˆ†
        if not HAS_LANGCHAIN:
            chunks = []
            for i in range(0, len(content), chunk_size - chunk_overlap):
                seg = content[i:i+chunk_size]
                chunks.append({
                    "source": file_meta.get("filename", "unknown"),
                    "text": seg,
                    "size": len(seg),
                    "weight": 1.0,
                    "metadata": {**file_meta, "source_type": source_type, "method": "legacy_split"}
                })
            print(f"   ğŸ”ª [Legacy Split] {file_meta.get('filename','?')} -> {len(chunks)} chunks")
            return chunks

        # MinerU Markdown: æ ‡é¢˜æ„ŸçŸ¥åˆ‡åˆ†
        if source_type == "mineru_markdown":
            try:
                headers_to_split_on = [
                    ("#", "Header 1"),
                    ("##", "Header 2"),
                    ("###", "Header 3"),
                ]
                md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
                md_docs = md_splitter.split_text(content)

                recursive_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                final_docs = recursive_splitter.split_documents(md_docs)
                chunks = _docs_to_chunks(final_docs, "mineru_markdown", file_meta)
                print(f"   ğŸ”ª [MinerU Split] {file_meta.get('filename','?')} -> {len(chunks)} chunks")
                return chunks
            except Exception as e:
                print(f"   âš ï¸ Markdown åˆ‡åˆ†å¤±è´¥ ({e})ï¼Œå›é€€æ ‡å‡†åˆ‡åˆ†")
                # ç»§ç»­èµ°æ ‡å‡†åˆ‡åˆ†

        # æ ‡å‡†é€’å½’åˆ‡åˆ†
        recursive_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        std_docs = recursive_splitter.create_documents([content], metadatas=[file_meta])
        chunks = _docs_to_chunks(std_docs, "standard_loader", file_meta)
        print(f"   ğŸ”ª [Standard Split] {file_meta.get('filename','?')} -> {len(chunks)} chunks")
        return chunks

    def _build_vector_index(self):
        """å¯é€‰: ä½¿ç”¨ TF-IDF å‘é‡åŒ–æå‡æ£€ç´¢æ•ˆæœ"""
        if not self.use_tfidf:
            return
        
        # ç¬¬ä¸€æ­¥ï¼šå°è¯•å¯¼å…¥ sklearn
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            has_sklearn = True
        except ImportError as ie:
            print(f"â„¹ï¸ æœªå®‰è£… sklearn ({ie})ï¼Œå‘é‡æ£€ç´¢å…³é—­ï¼Œç»§ç»­ä½¿ç”¨å…³é”®è¯åŒ¹é…")
            self.use_tfidf = False
            self.vectorizer = None
            self.tfidf_matrix = None
            return
        
        # ç¬¬äºŒæ­¥ï¼šå¦‚æœæˆåŠŸå¯¼å…¥ï¼Œç»§ç»­å‘é‡åŒ–å¤„ç†
        texts = [c.get("text", "") for c in self.chunks if c.get("text")]
        if not texts:
            print("âš ï¸ æ— æœ‰æ•ˆæ–‡æœ¬å†…å®¹ç”¨äº TF-IDF ç´¢å¼•")
            self.use_tfidf = False
            self.vectorizer = None
            self.tfidf_matrix = None
            return
        
        try:
            self.vectorizer = TfidfVectorizer(max_features=50000)
            self.tfidf_matrix = self.vectorizer.fit_transform(texts)
            print(f"âœ… å·²æ„å»º TF-IDF å‘é‡ç´¢å¼•: æ–‡æ¡£æ•° {len(texts)}, ç»´åº¦ {self.tfidf_matrix.shape[1]}")
        except Exception as e:
            print(f"âš ï¸ TF-IDF æ„å»ºå¤±è´¥ï¼Œå›é€€å…³é”®è¯æ£€ç´¢: {type(e).__name__}: {str(e)[:60]}")
            self.vectorizer = None
            self.tfidf_matrix = None
            self.use_tfidf = False

    def retrieve(self, query, top_k=6):
        """æ”¯æŒä¸­æ–‡çš„æ£€ç´¢é€»è¾‘ï¼Œå¸¦æƒé‡è®¡ç®—ï¼Œå¹¶æ ¼å¼åŒ–å…ƒæ•°æ®ï¼ˆä¼˜å…ˆå‘é‡æ£€ç´¢ï¼Œå›é€€å…³é”®è¯ï¼‰"""
        if not self.chunks: 
            return ""
        
        # ä¿®å¤ï¼šä½¿ç”¨æ”¯æŒ CJK çš„æ­£åˆ™è¿›è¡Œåˆ†è¯
        # åŒ¹é…ï¼šè¿ç»­çš„æ±‰å­— OR è¿ç»­çš„å­—æ¯æ•°å­—
        query_words = set(re.findall(r"[\u4e00-\u9fff]+|[a-zA-Z0-9]+", query))
        
        if not query_words and not query:
            return ""

        # å…³é”®è¯åŒ¹é…å¾—åˆ†
        keyword_scores = {}
        for idx, c in enumerate(self.chunks):
            score = 0
            for w in query_words:
                if w in c['text']:
                    score += 1
            if score > 0:
                keyword_scores[idx] = score * c.get('weight', 1.0)

        combined = []

        # TF-IDF å‘é‡æ£€ç´¢ï¼ˆå¯é€‰ï¼Œsklearn å¯èƒ½ä¸å¯ç”¨ï¼‰
        if self.use_tfidf and self.vectorizer is not None and self.tfidf_matrix is not None:
            try:
                q_vec = self.vectorizer.transform([query])
                if q_vec.nnz > 0:
                    sims = (self.tfidf_matrix @ q_vec.T).toarray().ravel()
                    for idx, sim in enumerate(sims):
                        if sim > 0:
                            kw_bonus = keyword_scores.get(idx, 0) * 0.1  # å…³é”®è¯è½»é‡åŠ æƒ
                            combined.append((sim + kw_bonus, idx))
                else:
                    # å‘é‡åŒ–ä¸ºç©ºæ—¶ï¼Œå›é€€å…³é”®è¯
                    pass
            except Exception as e:
                # sklearn ç›¸å…³çš„ä»»ä½•å¼‚å¸¸éƒ½å®‰å…¨é™çº§
                print(f"â„¹ï¸ å‘é‡æ£€ç´¢å¼‚å¸¸ ({type(e).__name__})ï¼Œè‡ªåŠ¨å›é€€å…³é”®è¯åŒ¹é…")
                self.use_tfidf = False
                self.vectorizer = None
                self.tfidf_matrix = None

        # å¦‚æœå‘é‡æ£€ç´¢ä¸å¯ç”¨æˆ–æœªå‘½ä¸­ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…
        if not combined:
            combined = [(score, idx) for idx, score in keyword_scores.items()]

        # æ’åºå¹¶æˆªæ–­
        combined.sort(key=lambda x: x[0], reverse=True)

        context = ""
        for rank, (s, idx) in enumerate(combined[:top_k], start=1):
            item = self.chunks[idx]
            meta = item.get("metadata", {}) or {}
            source = meta.get("filename", item.get("source", "unknown"))
            h1 = meta.get("Header 1") or meta.get("H1") or ""
            h2 = meta.get("Header 2") or meta.get("H2") or ""
            h3 = meta.get("Header 3") or meta.get("H3") or ""
            breadcrumb = source
            if h1: breadcrumb += f" > {h1}"
            if h2: breadcrumb += f" > {h2}"
            if h3: breadcrumb += f" > {h3}"
            context += (
                f"\n--- èµ„æ–™ç‰‡æ®µ {rank} (åŒ¹é…åº¦:{s:.3f}) ---\n"
                f"[æ¥æºè·¯å¾„]: {breadcrumb}\n"
                f"[å†…å®¹]: {item['text']}\n"
            )
        
        if context:
            print(f"      ğŸ§© [RAG] å‘½ä¸­ {len(combined[:top_k])} ä¸ªç‰‡æ®µ (å…³é”®è¯: {list(query_words)[:3]}...)")
        return context

# ================== ğŸŒ è”ç½‘æœç´¢ (å¸¦å®¹é”™) ==================

_TAVILY_KEY_WARNED = False

def search_web(query, force=False, cache_dir=None, max_retries=3):
    """
    å¢å¼ºç‰ˆè”ç½‘æœç´¢ï¼š
    - ç¨³å®šç¼“å­˜ (md5) è·¨è¿›ç¨‹å‘½ä¸­
    - æ”¯æŒä»£ç†
    - æŒ‡æ•°é€€é¿é‡è¯•
    - ç¼ºå¤± Key æ—¶ä»…è­¦å‘Šä¸€æ¬¡
    """
    global _TAVILY_KEY_WARNED

    api_key = os.getenv("TAVILY_API_KEY") or getattr(CONF, "TAVILY_API_KEY", "")
    if not api_key:
        if not _TAVILY_KEY_WARNED:
            print("âš ï¸ [Search] æœªæ£€æµ‹åˆ° TAVILY_API_KEYï¼Œç½‘ç»œæœç´¢å°†è·³è¿‡")
            _TAVILY_KEY_WARNED = True
        return ""
    if not query:
        return ""

    # é€‰æ‹©ç¼“å­˜ç›®å½•ï¼ˆä¼ å…¥ä¼˜å…ˆï¼Œå…¶æ¬¡å…¨å±€é»˜è®¤ï¼‰
    cache_root = cache_dir or os.path.join(os.path.expanduser("~"), "mineru", "workflow", "search_cache")
    os.makedirs(cache_root, exist_ok=True)

    # Fix: ä½¿ç”¨ç¨³å®šå“ˆå¸Œé¿å…é‡å¯å¤±æ•ˆ
    cache_file = os.path.join(cache_root, f"{hashlib.md5(query.encode('utf-8')).hexdigest()}.json")
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached = json.load(f)
            if time.time() - cached.get('timestamp', 0) < 86400:
                print("   ğŸ’¾ [ç¼“å­˜] ä½¿ç”¨å·²ç¼“å­˜æœç´¢ç»“æœ")
                return cached.get('result', '')
        except Exception as e:
            print(f"   âš ï¸ ç¼“å­˜è¯»å–å¤±è´¥ï¼Œé‡æ–°æœç´¢: {e}")

    # éå¼ºåˆ¶æ—¶çš„ç®€å•å…œåº•é€»è¾‘ï¼ˆä¿æŒæ—§è¡Œä¸ºï¼‰
    if not force:
        urgent_keywords = ['æ•°æ®', 'ç»Ÿè®¡', 'æœ€æ–°', '2024', '2025', 'æŠ¥å‘Š', 'æŒ‡æ•°', 'æ’å', 'åˆ†æ', 'ç°çŠ¶']
        if not any(kw in query for kw in urgent_keywords):
            return ""

    proxies = getattr(CONF, "PROXIES_CLOUD", None) or getattr(CONF, "PROXIES_LOCAL", None)
    url = "https://api.tavily.com/search"
    payload = {
        "api_key": api_key,
        "query": query,
        "search_depth": "advanced",
        "include_answer": True,
        "max_results": 5
    }

    for attempt in range(max_retries):
        try:
            print(f"   ğŸŒ [è”ç½‘æœç´¢] æŸ¥è¯¢: {query} (å°è¯• {attempt+1}/{max_retries})")
            res = requests.post(url, json=payload, headers={"Content-Type": "application/json"}, proxies=proxies, timeout=20)
            if res.status_code != 200:
                if res.status_code == 403:
                    print("   âŒ Tavily Key æ— æ•ˆæˆ–é¢åº¦è€—å°½")
                    return ""
                raise ConnectionError(f"Status {res.status_code}: {res.text[:200]}")

            data = res.json()
            answer = data.get("answer", "")
            details = [r.get("content", "") for r in data.get("results", [])]
            combined = "Answer: " + answer + "\n\nDetails:\n" + "\n".join(details)

            try:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump({"query": query, "result": combined, "timestamp": time.time()}, f, ensure_ascii=False)
            except Exception:
                pass

            return combined
        except Exception as e:
            if attempt < max_retries - 1:
                wait = 2 ** attempt
                print(f"   âš ï¸ æœç´¢å¤±è´¥ ({e})ï¼Œ{wait}s åé‡è¯•...")
                time.sleep(wait)
            else:
                print(f"   âŒ æœç´¢å½»åº•å¤±è´¥: {e}")
                return ""

# ================== ğŸ› ï¸ é€šç”¨è¯·æ±‚ (å¸¦é‡è¯•) ==================

def call_api_robust(url, payload, headers=None, max_retries=3, proxies=None, timeout=600):
    """å¸¦é‡è¯•çš„ POST è¯·æ±‚åŒ…è£…ï¼Œè¿”å›è§£æåçš„ JSON æˆ– None"""
    headers = headers or {}
    backoff = 2
    for attempt in range(max_retries):
        try:
            res = requests.post(url, json=payload, headers=headers, proxies=proxies, timeout=timeout)
            if res.status_code == 200:
                return res.json()
            if res.status_code in (429, 500, 502, 503, 504):
                print(f"   âš ï¸ API ç¹å¿™/å¼‚å¸¸ ({res.status_code})ï¼Œ{backoff}s åé‡è¯•...")
                time.sleep(backoff)
                backoff *= 2
                continue
            print(f"   âŒ API é”™è¯¯ ({res.status_code}): {res.text[:200]}")
            return None
        except Exception as e:
            print(f"   âš ï¸ è¿æ¥å¼‚å¸¸: {e}")
            time.sleep(backoff)
            backoff *= 2
    return None

# ================== ğŸ§  è´¨é‡è¯„ä¼°ä¸æ”¹è¿› ==================

def validate_json_chart_data(chart_data):
    """
    ã€æ•°æ®éªŒè¯å¼•æ“ã€‘ æ£€æŸ¥ JSON å›¾è¡¨æ•°æ®çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§
    è¿”å› (is_valid, error_messages)
    """
    errors = []
    
    # 1. åŸºç¡€å­—æ®µæ£€æŸ¥
    if 'chart_type' not in chart_data:
        errors.append("ç¼ºå°‘ chart_type å­—æ®µ")
    elif chart_data['chart_type'] not in ['bar', 'line', 'pie', 'radar', 'mixed']:
        errors.append(f"æ— æ•ˆçš„ chart_type: {chart_data['chart_type']}")
    
    if 'title' not in chart_data or not chart_data['title']:
        errors.append("ç¼ºå°‘ title å­—æ®µ")
    
    if 'data' not in chart_data:
        errors.append("ç¼ºå°‘ data å­—æ®µ")
        return False, errors
    
    data = chart_data['data']
    
    # 2. æ•°æ®ç»“æ„æ£€æŸ¥
    if 'labels' not in data or not data['labels']:
        errors.append("ç¼ºå°‘æˆ–ç©ºçš„ labels")
    
    if 'datasets' not in data or not data['datasets']:
        errors.append("ç¼ºå°‘æˆ–ç©ºçš„ datasets")
        return False, errors
    
    labels = data.get('labels', [])
    datasets = data.get('datasets', [])
    
    # 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    for i, ds in enumerate(datasets):
        if 'values' not in ds or not ds['values']:
            errors.append(f"datasets[{i}] ç¼ºå°‘ values æˆ–ä¸ºç©º")
            continue
        
        if len(ds['values']) != len(labels):
            errors.append(f"datasets[{i}] çš„å€¼ä¸ªæ•°({len(ds['values'])}) != labelsä¸ªæ•°({len(labels)})")
        
        # æ£€æŸ¥æ•°å€¼ç±»å‹
        for j, v in enumerate(ds['values']):
            try:
                float(v)
            except (TypeError, ValueError):
                errors.append(f"datasets[{i}].values[{j}] ä¸æ˜¯æœ‰æ•ˆæ•°å­—: {v}")
    
    # 4. æ•°æ®èŒƒå›´æ£€æŸ¥
    if errors:
        return False, errors
    
    # å¦‚æœæ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œè¿”å› True
    return True, []


def validate_content_structure(content):
    """
    ã€å†…å®¹ç»“æ„éªŒè¯ã€‘ æ£€æŸ¥ Markdown å†…å®¹çš„ç»“æ„å®Œæ•´æ€§
    è¿”å› (is_valid, suggestions)
    """
    suggestions = []
    
    # 1. æ£€æŸ¥æ ‡é¢˜ç»“æ„
    h3_headers = content.count('###')
    h4_headers = content.count('####')
    
    if h3_headers < 1:
        suggestions.append("å»ºè®®æ·»åŠ  ### ä¸‰çº§æ ‡é¢˜æ¥ç»„ç»‡å†…å®¹")
    
    # 2. æ£€æŸ¥è¡¨æ ¼
    table_lines = [l for l in content.split('\n') if '|' in l and l.count('|') >= 3]
    if len(table_lines) < 2:
        suggestions.append("å»ºè®®æ·»åŠ  Markdown è¡¨æ ¼æ±‡æ€»æ•°æ®")
    
    # 3. æ£€æŸ¥JSONå›¾è¡¨
    if '```json' not in content or '"chart_type"' not in content:
        suggestions.append("å»ºè®®æ·»åŠ  JSON å›¾è¡¨æ•°æ®å—")
    
    # 4. æ£€æŸ¥æ•°æ®æ¥æº
    if '[' not in content and 'http' not in content:
        suggestions.append("å»ºè®®ä¸ºå…³é”®æ•°æ®æ ‡æ³¨æ¥æº")
    
    is_valid = len(suggestions) == 0
    return is_valid, suggestions


# ================== ğŸ“ æ•°æ®é©±åŠ¨çš„å†™ä½œå¢å¼º ===================

def extract_data_points_from_content(content):
    """
    ä»å†…å®¹ä¸­æå–æ•°æ®ç‚¹å’Œæ¥æº
    è¿”å› [(æ•°æ®, æ¥æº), ...]
    """
    import re
    
    data_points = []
    
    # æå–æ•°å­— + å•ä½ï¼ˆå¦‚ "23.5%", "Â¥100ä¸‡", "2024å¹´"ï¼‰
    patterns = [
        r'(\d+(?:\.\d+)?%)',  # ç™¾åˆ†æ¯”
        r'([Â¥$â‚¬]\s*\d+(?:\.\d+)?(?:ä¸‡|äº¿|åƒ)?)',  # è´§å¸
        r'(\d{4}å¹´)',  # å¹´ä»½
        r'(CAGR\s*[\d.]+%)',  # CAGR
        r'(å¢é•¿\s*[\d.]+%)',  # å¢é•¿ç‡
    ]
    
    for pattern in patterns:
        matches = re.finditer(pattern, content)
        for match in matches:
            # å°è¯•æ‰¾åˆ°æ¥æºæ ‡æ³¨ [xxx]
            start = max(0, match.start() - 100)
            context = content[start:match.end() + 50]
            source_match = re.search(r'\[([^\]]+)\]', context)
            source = source_match.group(1) if source_match else "æœªæ ‡æ³¨"
            data_points.append((match.group(1), source))
    
    return data_points


def evaluate_content_quality(content, topic, section_title):
    """
    ã€ä¼ä¸šçº§è¯„ä¼°ç³»ç»Ÿ v2.0ã€‘ è¿”å› (score, feedback, improvement_hints)
    æ ‡å‡†ï¼šè®ºæ–‡çº§åˆ«ï¼ˆé€»è¾‘ä¸¥å¯†ã€è¯æ®å……åˆ†ã€ç»“æ„å®Œå–„ï¼‰
    è¯„åˆ†ä½“ç³»ï¼š0-10åˆ†ï¼Œç»†ç²’åº¦åé¦ˆ
    """
    if not content or len(content) < 300:
        return 0, "å†…å®¹è¿‡çŸ­", ["è¯·ç”Ÿæˆè‡³å°‘300å­—çš„å†…å®¹"]
    
    issues = []
    score = 10.0
    improvement_hints = []
    
    # ========== 1. å†…å®¹æ·±åº¦ä¸ç¯‡å¹… (20%) ==========
    word_count = len(content)
    if word_count < 800:
        issues.append(f"ç¯‡å¹…æµ…è–„ ({word_count}å­— < 800å­—)")
        score -= 2.0
        improvement_hints.append("éœ€è¦å±•å¼€è®ºè¿°ï¼Œè‡³å°‘è¡¥å……400å­—ä»¥ä¸Šå†…å®¹")
    elif word_count < 1200:
        issues.append(f"ç¯‡å¹…ä¸­ç­‰ ({word_count}å­—)")
        score -= 0.5
        improvement_hints.append("å»ºè®®è¡¥å……æ¡ˆä¾‹æˆ–å¯¹æ ‡åˆ†æï¼Œè¾¾åˆ°1200å­—ä»¥ä¸Š")
    
    # ========== 2. é€»è¾‘ç»“æ„ä¸å±‚çº§ (20%) ==========
    h3_count = content.count("###")
    h4_count = content.count("####")
    total_headers = h3_count + h4_count
    
    if total_headers == 0:
        issues.append("æ— é€»è¾‘åˆ†å±‚")
        score -= 2.5
        improvement_hints.append("æ·»åŠ ### å’Œ #### æ ‡é¢˜è¿›è¡Œé€»è¾‘åˆ†å±‚ï¼ˆè‡³å°‘2-3å±‚ï¼‰")
    elif total_headers == 1:
        issues.append("é€»è¾‘å±‚çº§å•è–„")
        score -= 1.0
        improvement_hints.append("è¡¥å……æ›´å¤šå­æ ‡é¢˜ï¼Œå½¢æˆæ ‘çŠ¶ç»“æ„")
    
    # æ£€æŸ¥æ®µè½é•¿åº¦ï¼ˆè¿‡é•¿æ®µè½å½±å“å¯è¯»æ€§ï¼‰
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    avg_para_len = word_count / max(len(paragraphs), 1)
    if avg_para_len > 400:
        issues.append(f"æ®µè½è¿‡é•¿ (å¹³å‡{avg_para_len:.0f}å­—)")
        score -= 0.8
        improvement_hints.append("å»ºè®®æ‹†åˆ†é•¿æ®µè½ï¼Œæ¯æ®µä¿æŒ200-300å­—")
    
    # ========== 3. æ•°æ®è¯æ® (25%) ==========
    digit_count = sum(c.isdigit() for c in content)
    data_ratio = digit_count / len(content) if len(content) > 0 else 0
    
    if data_ratio < 0.02:  # æé«˜åˆ°2%
        issues.append(f"æ•°æ®å¯†åº¦è¿‡ä½ ({data_ratio*100:.1f}% < 2%)")
        score -= 2.5
        improvement_hints.append("è¡¥å……å…·ä½“æ•°å­—ã€ç™¾åˆ†æ¯”ã€å‚æ•°ç­‰å®šé‡æ•°æ®ï¼ˆæ¯300å­—è‡³å°‘1ä¸ªæ•°æ®ç‚¹ï¼‰")
    elif data_ratio < 0.03:
        score -= 0.5
        improvement_hints.append("æ•°æ®å¯†åº¦å¯è¿›ä¸€æ­¥æå‡ï¼Œç›®æ ‡3-5%")
    
    # æ£€æŸ¥å¼•ç”¨æ ¼å¼ï¼ˆæ•°æ®æ¥æºï¼‰
    has_citation = "[" in content and "]" in content
    has_url = "http" in content
    if not has_citation and not has_url:
        issues.append("ç¼ºå°‘æ•°æ®æ¥æºæ ‡æ³¨")
        score -= 1.0
        improvement_hints.append("ä¸ºå…³é”®æ•°æ®æ ‡æ³¨æ¥æº [æ¥æº] æˆ– URL")
    
    # ========== 4. å¯è§†åŒ–ä¸è¡¨æ ¼ (20%) ==========
    # æ›´ä¸¥æ ¼çš„è¡¨æ ¼æ£€æµ‹
    table_lines = [l for l in content.split('\n') if '|' in l and len(l.split('|')) >= 4]
    has_quality_table = len(table_lines) >= 2
    has_json_chart = "```json" in content and "chart_type" in content
    
    if not has_quality_table and not has_json_chart:
        issues.append("ä¸¥é‡ç¼ºå¤±å¯è§†åŒ–")
        score -= 3.0
        improvement_hints.append("å¿…é¡»åŒ…å« Markdown è¡¨æ ¼æˆ– JSON å›¾è¡¨æ•°æ®å—")
    elif not has_json_chart:
        score -= 1.5
        improvement_hints.append("å»ºè®®è¡¥å…… JSON å›¾è¡¨å—ä¾¿äºç”Ÿæˆä¸“ä¸šå›¾è¡¨")
    
    # ========== 5. ä¸“ä¸šæœ¯è¯­å¯†åº¦ (15%) ==========
    professional_keywords = {
        "æŠ€æœ¯": ["ç®—æ³•", "æ¶æ„", "æ¨¡å‹", "æ¡†æ¶", "åè®®", "æ¥å£", "å‚æ•°", "ä¼˜åŒ–"],
        "å•†ä¸š": ["å¸‚åœº", "ç«äº‰", "æˆæœ¬", "æ•ˆç›Š", "æ”¶ç›Š", "é£é™©", "æˆ˜ç•¥", "æ–¹æ¡ˆ"],
        "å­¦æœ¯": ["ç ”ç©¶", "åˆ†æ", "è®ºè¯", "å®è¯", "ç†è®º", "å‡è®¾", "ç»“è®º"],
        "è§„èŒƒ": ["æ ‡å‡†", "è§„èŒƒ", "ç¬¦åˆ", "GB", "ISO", "è§„å®š", "è¦æ±‚"]
    }
    
    prof_count = 0
    for category, keywords in professional_keywords.items():
        prof_count += sum(content.count(kw) for kw in keywords)
    
    prof_ratio = prof_count / max(word_count / 100, 1)  # æ¯100å­—æœŸæœ›1ä¸ªä¸“ä¸šè¯æ±‡
    
    if prof_ratio < 0.5:
        issues.append(f"ä¸“ä¸šæœ¯è¯­ä¸è¶³ ({prof_ratio:.1f}/100å­—)")
        score -= 1.5
        improvement_hints.append("å¢åŠ é¢†åŸŸç‰¹å®šçš„ä¸“ä¸šæœ¯è¯­å’Œè¡Œä¸šç”¨è¯­")
    elif prof_ratio < 1.0:
        score -= 0.3
        improvement_hints.append("ä¸“ä¸šæœ¯è¯­ä½¿ç”¨å¯è¿›ä¸€æ­¥å¢å¼º")
    
    # ========== 6. è®ºç‚¹æ”¯æ’‘åŠ› (5%) ==========
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹æ¯”ã€å› æœã€é€’è¿›ç­‰é€»è¾‘è¯
    logic_connectors = ["å› æ­¤", "æ‰€ä»¥", "ç›¸æ¯”", "ä¸ä¹‹ç›¸å¯¹", "è¿›è€Œ", "ç”±æ­¤å¯è§", "æ ¹æ®", "è¯æ˜", "è¡¨æ˜"]
    logic_count = sum(content.count(conn) for conn in logic_connectors)
    
    if logic_count == 0:
        issues.append("ç¼ºä¹è®ºè¯é€»è¾‘")
        score -= 1.0
        improvement_hints.append("ä½¿ç”¨é€»è¾‘è¯å¼ºåŒ–è®ºè¯ï¼š'å› æ­¤'ã€'ç›¸æ¯”'ã€'å› æ­¤'ç­‰")
    
    # ========== æœ€ç»ˆåé¦ˆ ==========
    feedback_text = " | ".join(issues) if issues else "âœ… ä¼˜ç§€ï¼šç»“æ„å®Œå–„ã€æ•°æ®å……å®ã€è¡¨è¿°ä¸“ä¸š"
    return max(0.0, score), feedback_text, improvement_hints


def generate_refinement_prompt(original_prompt, content, quality_score, feedback, improvement_hints, round_num):
    """
    ã€ä¼ä¸šçº§æ”¹è¿›ç³»ç»Ÿ v2.0ã€‘åŸºäºè´¨é‡è¯„ä¼°åŠ¨æ€ç”Ÿæˆæ”¹è¿›æç¤ºè¯
    æ”¯æŒå¤šè½®è¿­ä»£ï¼Œæ¯è½®èšç„¦ä¸åŒç»´åº¦
    """
    if quality_score >= 8.5:
        return None  # è´¨é‡ä¼˜ç§€ï¼Œæ— éœ€æ”¹è¿›
    
    # æ ¹æ®æ”¹è¿›è½®æ¬¡é‡‡ç”¨ä¸åŒç­–ç•¥
    if round_num == 1:
        # ç¬¬1è½®ï¼šå¿«é€Ÿæ”¹è¿› - ä¸“æ³¨ç¯‡å¹…å’Œæ•°æ®å¯†åº¦
        focus = """
        ã€ç¬¬1è½®æ”¹è¿›ï¼šå¿«é€Ÿæ·±åŒ–ã€‘
        å½“å‰è¯„åˆ†: {}/10
        é—®é¢˜æ±‡æ€»: {}
        
        æ”¹è¿›é‡ç‚¹ (ä¼˜å…ˆçº§é¡ºåº):
        1. ç¯‡å¹…ä¸æ·±åº¦: 
           - ç›®æ ‡: 1000+ å­—ï¼ˆæ¯ä¸ªå­èŠ‚ï¼‰
           - æ–¹æ³•: è¡¥å……è¯¦å°½çš„åˆ†ææ®µè½ã€æ¡ˆä¾‹è®ºè¯ã€å¯¹æ¯”åˆ†æ
           
        2. æ•°æ®å¯†åº¦:
           - ç›®æ ‡: æ•°æ®å æ¯” 3-5%ï¼ˆå…·ä½“æ•°å­—ã€ç™¾åˆ†æ¯”ã€å‚æ•°ï¼‰
           - æ–¹æ³•: åŠ å…¥ CAGRã€å¸‚åœºä»½é¢ã€æŠ€æœ¯å‚æ•°ã€æˆæœ¬æ•°æ®ç­‰
           - æ ¼å¼: åœ¨æ•°æ®åæ ‡æ³¨ [æ¥æº:xxx] æˆ– [æ•°æ®å¹´ä»½:xxxx]
           
        3. å¯è§†åŒ–:
           - å¿…é¡»: ä¸€ä¸ªé«˜è´¨é‡ Markdown è¡¨æ ¼ï¼ˆæ±‡æ€»æ•°æ®ï¼‰
           - å¿…é¡»: ä¸€ä¸ª JSON å›¾è¡¨æ•°æ®å—ï¼ˆä¾¿äºç”Ÿæˆä¸“ä¸šå›¾ï¼‰
        """.format(quality_score, feedback)
        
    elif round_num == 2:
        # ç¬¬2è½®ï¼šé€»è¾‘ä¼˜åŒ– - èšç„¦ç»“æ„å’Œè®ºè¯
        focus = """
        ã€ç¬¬2è½®æ”¹è¿›ï¼šé€»è¾‘å¼ºåŒ–ã€‘
        å½“å‰è¯„åˆ†: {}/10
        æ”¹è¿›æç¤º: {}
        
        æ”¹è¿›é‡ç‚¹ (ä¼˜å…ˆçº§é¡ºåº):
        1. é€»è¾‘ç»“æ„:
           - æ·»åŠ  ### å’Œ #### å­æ ‡é¢˜ï¼ˆè‡³å°‘3å±‚ç»“æ„ï¼‰
           - æ¯ä¸ªæ®µè½ä¸è¶…è¿‡300å­—ï¼Œæ®µè½é—´ç”¨é€»è¾‘è¯è¿æ¥
           - ä½¿ç”¨é€»è¾‘è¯: å› æ­¤ã€ç›¸æ¯”ã€ä¸ä¹‹ç›¸å¯¹ã€ç”±æ­¤å¯è§ã€æ ¹æ®ã€è¯æ˜ç­‰
           
        2. è®ºè¯å®Œå–„:
           - æ¯ä¸ªé‡è¦è®ºç‚¹åè·Ÿå…·ä½“è¯æ®ï¼ˆæ•°æ®ã€æ¡ˆä¾‹ã€å¼•ç”¨ï¼‰
           - é‡‡ç”¨é—®é¢˜-åˆ†æ-ç»“è®ºçš„ç»“æ„
           - é¿å…ç©ºæ³›è¡¨è¿°ï¼Œæ‰€æœ‰è§‚ç‚¹å¿…é¡»æœ‰ä¾æ®
           
        3. ä¸“ä¸šæœ¯è¯­:
           - èå…¥é¢†åŸŸç‰¹æœ‰æœ¯è¯­ï¼Œä½†ä¿æŒè¡¨è¾¾é€šä¿—æ˜“æ‡‚
           - é‡è¦æ¦‚å¿µé¦–æ¬¡å‡ºç°æ—¶åŠ æ‹¬å·è‹±æ–‡è§£é‡Š
        """.format(quality_score, "\n".join(improvement_hints[:2]))
        
    elif round_num >= 3:
        # ç¬¬3è½®ï¼šç²¾ç»†æ‰“ç£¨ - ä¸“ä¸šåº¦æå‡
        focus = """
        ã€ç¬¬3è½®æ”¹è¿›ï¼šç²¾ç»†æ‰“ç£¨ã€‘
        å½“å‰è¯„åˆ†: {}/10
        
        æ”¹è¿›é‡ç‚¹ (ä¼˜å…ˆçº§é¡ºåº):
        1. ä¸“ä¸šå®¡ç¾æå‡:
           - è¡¨æ ¼æ ¼å¼è§„èŒƒï¼šè¡¨å¤´åŠ ç²—ã€å¯¹é½ã€å•ä½ç»Ÿä¸€
           - æ•°æ®è¡¨è¿°ï¼šé¿å…"å¤§å¹…å¢é•¿"ï¼Œæ”¹ç”¨"åŒæ¯”å¢é•¿ 23.5%"
           - ä¸­è‹±æ–‡æ··ç”¨ï¼šæŠ€æœ¯æœ¯è¯­ç”¨è‹±æ–‡ï¼Œä¸šåŠ¡ç”¨ä¸­æ–‡
           
        2. å†…å®¹å®Œå–„:
           - è¡¥å……å¯¹æ ‡æ¡ˆä¾‹æˆ–è¡Œä¸šå…¸èŒƒ
           - åŠ å…¥é£é™©è¯†åˆ«æˆ–å±€é™æ€§åˆ†æ
           - æå‡ºå¯è¡Œçš„æ”¹è¿›å»ºè®®æˆ–ä¸‹ä¸€æ­¥æ–¹å‘
           
        3. å¯è¯»æ€§ä¼˜åŒ–:
           - å¤æ‚æ¦‚å¿µå‰åŠ "ç®€è¨€ä¹‹"çš„æµ“ç¼©è¡¨è¿°
           - æ®µè½é—´é€»è¾‘é€’è¿›è‡ªç„¶ï¼Œé¿å…ç”Ÿç¡¬
           - ä¿ç•™åŸæœ‰ä¼˜è´¨å†…å®¹ï¼Œåªè¡¥å……ä¸åˆ å‡
        """.format(quality_score)
    else:
        focus = "åŸºäºåé¦ˆè¡¥å……æ”¹è¿›"
    
    refinement_prompt = f"""
    {original_prompt}
    
    ================== å¤šè½®è¿­ä»£æ”¹è¿› ==================
    {focus}
    
    ã€å¿…é¡»éµå®ˆçš„æ ¼å¼è¦æ±‚ã€‘:
    1. è¿”å›å®Œæ•´çš„æ”¹è¿›åå†…å®¹ï¼ˆä¸æ˜¯ä¿®æ”¹è¯´æ˜ï¼‰
    2. ä¿ç•™åŸæœ‰çš„å¥½å†…å®¹ï¼Œåªè¡¥å……ä¸åˆ å‡
    3. ä¿æŒ Markdown æ ¼å¼ä¸€è‡´
    4. åŒ…å«è¡¨æ ¼æ—¶ï¼Œè¡Œæ•° >= 4ï¼ˆè¡¨å¤´+æ•°æ®è¡Œï¼‰
    5. åŒ…å« JSON å›¾è¡¨æ•°æ®æ—¶ï¼Œç¡®ä¿è¯­æ³•æ­£ç¡®ï¼ˆæ— æ³¨é‡Šï¼‰
    """
    return refinement_prompt


def generate_enhanced_prompt_with_visuals(base_prompt, topic, section_title):
    """
    ã€å‡çº§ç‰ˆã€‘ç”Ÿæˆå¢å¼ºçš„æç¤ºè¯
    è§’è‰²ï¼šè¡Œä¸šé¢†åŸŸèµ„æ·±ä¸“å®¶ (Industry Domain Senior Expert)
    ç­–ç•¥ï¼šCoT (æ€ç»´é“¾) + æ·±åº¦è¡Œä¸šæ´å¯Ÿ + å¼ºåˆ¶ JSON æ•°æ®è¾“å‡º
    """
    enhanced = f"""
    {base_prompt}
    
    ========== ğŸ›ï¸ è§’è‰²ä¸æ ‡å‡† ==========
    ä½ ç°åœ¨æ˜¯ã€{topic}ã€‘é¢†åŸŸçš„ã€é¦–å¸­è¡Œä¸šä¸“å®¶ã€‘(Chief Industry Expert)ã€‚
    ä½ æ‹¥æœ‰30å¹´çš„è¡Œä¸šå®æˆ˜ç»éªŒï¼Œå¯¹è¯¥é¢†åŸŸçš„æŠ€æœ¯æ¼”è¿›ã€å¸‚åœºæ ¼å±€ã€æ”¿ç­–èµ°å‘æœ‰ææ·±çš„æ´å¯Ÿã€‚
    
    ä½ çš„ä»»åŠ¡æ˜¯æ’°å†™ä¸€ä»½å…·æœ‰ã€è¡Œä¸šæ ‡æ†æ°´å‡†ã€‘çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Š/æŠ€æœ¯æ–¹æ¡ˆã€‚
    
    ã€ä¸“å®¶çº§å†™ä½œæ ‡å‡†ã€‘ï¼š
    1. **æ·±åº¦æ´å¯Ÿ (Deep Insight)**ï¼š
       - ä¸è¦åªç½—åˆ—ç°è±¡ï¼Œè¦æŒ–æ˜èƒŒåçš„æ ¹æœ¬åŸå›  (Root Cause) å’Œåº•å±‚é€»è¾‘ã€‚
       - èƒ½å¤Ÿè¯†åˆ«è¡Œä¸šç—›ç‚¹ï¼Œå¹¶æå‡ºåˆ‡å®å¯è¡Œçš„ã€å…·æœ‰å‰ç»æ€§çš„è§£å†³æ–¹æ¡ˆã€‚
       
    2. **æŠ€æœ¯æƒå¨ (Technical Authority)**ï¼š
       - ç†Ÿç»ƒå¼•ç”¨æœ€æ–°çš„è¡Œä¸šæ ‡å‡† (GB/ISO/IEC)ã€ä¸“åˆ©æŠ€æœ¯æˆ–å­¦æœ¯å‰æ²¿æˆæœã€‚
       - ä½¿ç”¨è¯¥é¢†åŸŸæœ€åœ°é“çš„è¡Œè¯ (Jargon)ï¼Œä½†è¦è§£é‡Šæ¸…æ¥šå…¶å•†ä¸šä»·å€¼ã€‚
       
    3. **æ•°æ®é©±åŠ¨ (Data-Driven)**ï¼š
       - æ¯ä¸€ä¸ªè®ºç‚¹éƒ½å¿…é¡»æœ‰å…·ä½“çš„æ•°æ®æ”¯æ’‘ï¼ˆå¦‚ï¼šCAGRã€å¸‚åœºæ¸—é€ç‡ã€æŠ€æœ¯å‚æ•°ã€æˆæœ¬æ•ˆç›Šæ¯”ï¼‰ã€‚
       - æ‹’ç»æ¨¡ç³Šçš„â€œå¤§å¹…æå‡â€ï¼Œå¿…é¡»é‡åŒ–ä¸ºâ€œæå‡äº† 23.5%â€ã€‚
       
    4. **ç»“æ„ä¸¥è°¨ (Structured Thinking)**ï¼š
       - ä½¿ç”¨ MECE åŸåˆ™ï¼ˆç›¸äº’ç‹¬ç«‹ï¼Œå®Œå…¨ç©·å°½ï¼‰ç»„ç»‡å†…å®¹ã€‚
       - é‡‡ç”¨â€œç»“è®ºå…ˆè¡Œâ€çš„å’¨è¯¢é¡¾é—®å¼è¡¨è¾¾é£æ ¼ã€‚
    
    ========== ğŸ“Š å¼ºåˆ¶å¯è§†åŒ–è¦æ±‚ (JSON) ==========
    ä¸ºäº†ç”Ÿæˆä¸“ä¸šå›¾è¡¨ï¼Œä½ å¿…é¡»åœ¨å›ç­”æœ«å°¾æä¾›ä¸€ä¸ªã€æ ‡å‡†çš„ JSON æ•°æ®å—ã€‘ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    
    ```json
    {{
      "chart_type": "bar",  // å¯é€‰: "bar"(æŸ±çŠ¶), "line"(æŠ˜çº¿), "pie"(é¥¼å›¾), "radar"(é›·è¾¾å›¾), "mixed"(ç»„åˆ)
      "title": "2020-2024å¹´å¸‚åœºä»½é¢åˆ†æ",
      "data": {{
        "labels": ["2020", "2021", "2022", "2023", "2024"],
        "datasets": [
           {{ "label": "æœ¬å…¬å¸", "values": [20, 25, 30, 35, 40] }},
           {{ "label": "ç«äº‰å¯¹æ‰‹A", "values": [30, 28, 25, 20, 15] }}
        ]
      }},
      "x_label": "å¹´ä»½",
      "y_label": "å¸‚åœºä»½é¢ (%)"
    }}
    ```
    *æ³¨æ„ï¼šè¯·ç¡®ä¿ JSON è¯­æ³•å®Œå…¨æ­£ç¡®ï¼Œä¸è¦åœ¨ JSON ä¸­æ·»åŠ æ³¨é‡Šã€‚*
    
    ========== ğŸ“ ä¸“å®¶è§†è§’ç»“æ„å»ºè®® ==========
    1. **æ ¸å¿ƒè®ºç‚¹ (Thesis Statement)**: å¼€ç¯‡å³äº®å‡ºå…·æœ‰ä¸“å®¶è§†è§’çš„ç‹¬åˆ°è§è§£ã€‚
    2. **ç°çŠ¶ä¸æŒ‘æˆ˜ (Status & Challenges)**: æ·±åº¦å‰–æå½“å‰çš„æŠ€æœ¯/å¸‚åœºç“¶é¢ˆã€‚
    3. **è§£å†³æ–¹æ¡ˆ (Solution)**: æå‡ºç³»ç»Ÿæ€§çš„æŠ€æœ¯è·¯çº¿æˆ–æˆ˜ç•¥å»ºè®®ï¼Œå¼ºè°ƒåˆ›æ–°ç‚¹ã€‚
    4. **ä»·å€¼éªŒè¯ (Value Proposition)**: ç”¨æ•°æ®å’Œæ¡ˆä¾‹è¯æ˜æ–¹æ¡ˆçš„å¯è¡Œæ€§ä¸ä¼˜è¶Šæ€§ã€‚
    
    å…³é”®è¯ï¼š{topic}, {section_title}
    """
    return enhanced

def generate_enhanced_prompt_with_visuals(base_prompt, topic, section_title, attempt_num=1):
    """
    ã€ä¼ä¸šçº§æç¤ºè¯å·¥ç¨‹ v3.0ã€‘ 
    é‡‡ç”¨é«˜çº§ç­–ç•¥ï¼šè§’è‰²æ‰®æ¼” + CoT (æ€ç»´é“¾) + è¡Œä¸šæ ‡æ† + å¼ºåˆ¶è¾“å‡ºæ ¼å¼
    attempt_num: ç¬¬å‡ è½®å°è¯•ï¼ˆ1=åˆç¨¿ï¼Œ2+=æ”¹è¿›ï¼‰
    """
    
    # ========== åŠ¨æ€è°ƒæ•´ç­–ç•¥ ==========
    if attempt_num == 1:
        # åˆç¨¿ï¼šå¼ºè°ƒæ·±åº¦å’Œä¸“ä¸šåº¦
        thinking_depth = "æ·±åº¦åæ€"
        output_target = "è¡Œä¸šæ ‡æ†çº§"
    else:
        # æ”¹è¿›ç¨¿ï¼šå¼ºè°ƒæ•°æ®å’Œä¸¥è°¨
        thinking_depth = "è¯¦å°½è®ºè¯"
        output_target = "ä¸“ä¸šæœŸåˆŠçº§"
    
    enhanced = f"""
    ã€å‰ç½®ä»»åŠ¡åˆ†æã€‘
    ä½ å³å°†ç”Ÿæˆä¸€ä»½å…³äº ã€{topic}ã€‘ ä¸­ ã€{section_title}ã€‘ çš„ä¸“ä¸šå†…å®¹ã€‚
    è¿™ä»½å†…å®¹å°†ä½œä¸ºè¡Œä¸šç ”ç©¶æŠ¥å‘Šçš„é‡è¦ç« èŠ‚ï¼Œç›®æ ‡å—ä¼—æ˜¯è¡Œä¸šé«˜ç®¡ã€æŠ€æœ¯å†³ç­–è€…å’ŒæŠ•èµ„æ–¹ã€‚
    è´¨é‡æ ‡å‡†ï¼š{output_target}æ°´å¹³ã€‚
    
    ========== ğŸ›ï¸ è§’è‰²å®šä½ ==========
    ä½ æ˜¯è¯¥é¢†åŸŸçš„ã€é¦–å¸­ä¸“ä¸šå’¨è¯¢å¸ˆã€‘(Chief Professional Consultant)
    â€¢ æ‹¥æœ‰15+å¹´çš„è¡Œä¸šæ·±è€•ç»å†
    â€¢ æ›¾æ’°å†™å¤šä»½è¡Œä¸šæ ‡æ†ç ”ç©¶æŠ¥å‘Š
    â€¢ å¯¹å¸‚åœºã€æŠ€æœ¯ã€æ”¿ç­–æœ‰ç³»ç»Ÿæ€§è®¤çŸ¥
    â€¢ æ“…é•¿ç”¨æ•°æ®å’Œæ¡ˆä¾‹æ”¯æ’‘è®ºç‚¹
    
    ========== ğŸ§  {thinking_depth}æµç¨‹ (CoT - Chain of Thought) ==========
    åœ¨å¼€å§‹å†™ä½œå‰ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤{thinking_depth}ï¼š
    
    1. ã€èƒŒæ™¯æ¢³ç†ã€‘
       - è¿™ä¸ªä¸»é¢˜åœ¨è¡Œä¸šä¸­çš„å½“å‰çŠ¶æ€ï¼Ÿ
       - å­˜åœ¨å“ªäº›æ ¸å¿ƒé—®é¢˜å’Œç—›ç‚¹ï¼Ÿ
       - ä¸ºä»€ä¹ˆè¿™ä¸ªè¯é¢˜å¯¹ç›®æ ‡å—ä¼—é‡è¦ï¼Ÿ
    
    2. ã€è®ºç‚¹æ„å»ºã€‘
       - æ ¸å¿ƒè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼ˆä¸€å¥è¯æ€»ç»“ï¼‰ï¼Ÿ
       - æœ‰å“ªäº›æ”¯æ’‘è¿™ä¸ªè§‚ç‚¹çš„è¯æ®ï¼ˆæ•°æ®ã€æ¡ˆä¾‹ã€ç†è®ºï¼‰ï¼Ÿ
       - æ˜¯å¦å­˜åœ¨å¯¹ç«‹è§‚ç‚¹ï¼Ÿå¦‚ä½•é©³æ–¥æˆ–èåˆï¼Ÿ
    
    3. ã€ç»“æ„è§„åˆ’ã€‘
       - å¦‚ä½•åˆ†å±‚ç»„ç»‡å†…å®¹ï¼ˆ3-4å±‚é€»è¾‘ç»“æ„ï¼‰ï¼Ÿ
       - æ®µè½é¡ºåºæ˜¯å¦å½¢æˆè‡ªç„¶é€’è¿›ï¼Ÿ
       - æ˜¯å¦éœ€è¦å¯è§†åŒ–è¡¥å……ï¼Ÿ
    
    4. ã€è´¨é‡æ£€æŸ¥ã€‘
       - æ•°æ®å¯†åº¦æ˜¯å¦å……è¶³ï¼ˆ3-5%ï¼‰ï¼Ÿ
       - ä¸“ä¸šæœ¯è¯­ä½¿ç”¨æ˜¯å¦å¾—å½“ï¼Ÿ
       - é€»è¾‘è¿æ¥è¯æ˜¯å¦å……åˆ†ï¼Ÿ
    
    ========== ğŸ“ å†…å®¹å†™ä½œæ ‡å‡† ==========
    
    ã€æ·±åº¦æ´å¯Ÿ (Deep Insight)ã€‘
    â€¢ ä¸ä»…æè¿°ç°è±¡ï¼Œæ›´è¦åˆ†ææ ¹æœ¬åŸå› å’Œé©±åŠ¨å› ç´ 
    â€¢ è¯†åˆ«éšå«çš„è¡Œä¸šç—›ç‚¹å’Œæœºé‡
    â€¢ æå‡ºå‰ç»æ€§ã€å¯è¡Œæ€§çš„è§£å†³æ–¹æ¡ˆ
    
    ã€æ•°æ®é©±åŠ¨ (Data-Driven)ã€‘
    â€¢ æ¯ä¸ªé‡è¦è®ºç‚¹åå¿…é¡»æœ‰å…·ä½“æ•°æ®æ”¯æ’‘
    â€¢ æ•°æ®æ ¼å¼ï¼šå…·ä½“æ•°å­—ã€ç™¾åˆ†æ¯”ã€CAGRã€åŒæ¯”/ç¯æ¯”å¢é•¿
    â€¢ ç¤ºä¾‹ï¼š
      âŒ "å¸‚åœºå¢é•¿å¾ˆå¿«"
      âœ… "2023å¹´å¸‚åœºè§„æ¨¡ä¸º Â¥XX äº¿ï¼ŒCAGR ä¸º 23.5%ï¼Œé¢„è®¡2025å¹´è¾¾ Â¥YY äº¿"
    â€¢ æ•°æ®æ ‡æ³¨ï¼šåœ¨æ•°æ®ååŠ  [æ¥æº:XXX 2024å¹´æŠ¥] æˆ– [æ•°æ®æˆªè‡³2024å¹´10æœˆ]
    
    ã€æŠ€æœ¯æƒå¨ (Technical Authority)ã€‘
    â€¢ èå…¥è¡Œä¸šæ ‡å‡†ã€å›½é™…è§„èŒƒã€æŠ€æœ¯æ¡†æ¶
    â€¢ ä½¿ç”¨è¡Œä¸šç‰¹æœ‰æœ¯è¯­ï¼Œä½†ä¿æŒå¯ç†è§£
    â€¢ å¼•ç”¨å¦‚ GBã€ISOã€IECã€IEEEã€RFC ç­‰
    â€¢ ç¬¬ä¸€æ¬¡å‡ºç°æŠ€æœ¯æœ¯è¯­æ—¶ï¼Œå¯ç”¨æ‹¬å·è¡¥å……è‹±æ–‡
    
    ã€ç»“æ„ä¸¥è°¨ (Structured Thinking)ã€‘
    â€¢ é‡‡ç”¨MECEåŸåˆ™ï¼ˆç›¸äº’ç‹¬ç«‹ï¼Œå®Œå…¨ç©·å°½ï¼‰ç»„ç»‡è®ºç‚¹
    â€¢ ä½¿ç”¨"ç»“è®ºå…ˆè¡Œ"é£æ ¼ï¼šå…ˆè¯´ç»“è®ºï¼Œå†è¯´åŸå› 
    â€¢ æ®µè½é—´é€»è¾‘è¯è¿æ¥ï¼šå› æ­¤ã€ç›¸æ¯”ã€ä¸ä¹‹ç›¸å¯¹ã€è¿›è€Œã€ç”±æ­¤å¯è§ç­‰
    â€¢ Markdown ç»“æ„ï¼š
      # ä¸€çº§æ ‡é¢˜ï¼ˆç« åï¼‰
      ## äºŒçº§æ ‡é¢˜ï¼ˆèŠ‚åï¼‰
      ### ä¸‰çº§æ ‡é¢˜ï¼ˆæ ¸å¿ƒè®ºç‚¹ï¼‰
      #### å››çº§æ ‡é¢˜ï¼ˆåˆ†è®ºç‚¹ï¼‰
    
    ã€å¯è§†åŒ–ä¸°å¯Œ (Visual Rich)ã€‘
    â€¢ Markdown è¡¨æ ¼ï¼šæ±‡æ€»å…³é”®æ•°æ®æˆ–å¯¹æ¯”åˆ†æ
      - è¡Œæ•° >= 4ï¼ˆè¡¨å¤´+æ•°æ®è¡Œï¼‰
      - åˆ—æ•° >= 3ï¼ˆæ˜ç¡®çš„å¯¹æ¯”ç»´åº¦ï¼‰
      - è¡¨æ ¼å‰éœ€æœ‰è§£é‡Šæ–‡å­—
    
    â€¢ JSON å›¾è¡¨æ•°æ®å—ï¼šç”¨äºç”Ÿæˆä¸“ä¸šå›¾è¡¨
      - æ”¾åœ¨æ­£æ–‡æœ«å°¾ï¼Œ```json ... ``` æ ¼å¼
      - å¿…é¡»åŒ…å«ï¼šchart_type, title, data (labels, datasets), x_label, y_label
      - ã€é‡è¦ã€‘å°½é‡ä½¿ç”¨å¤šæ ·åŒ–å›¾è¡¨ç±»å‹ï¼Œä¸è¦é‡å¤ï¼š
        * bar - æŸ±çŠ¶å›¾ï¼ˆå¯¹æ¯”ã€æ’åºã€æ’åï¼‰
        * line - æŠ˜çº¿å›¾ï¼ˆè¶‹åŠ¿ã€æ—¶åºã€å¢é•¿ç‡ï¼‰
        * pie - é¥¼å›¾ï¼ˆå æ¯”ã€å¸‚åœºä»½é¢ã€æ„æˆï¼‰
        * radar - é›·è¾¾å›¾ï¼ˆå¤šç»´è¯„ä»·ã€æŠ€æœ¯å¯¹æ ‡ã€èƒ½åŠ›è¯„åˆ†ï¼‰
        * area - é¢ç§¯å›¾ï¼ˆå †ç§¯è¶‹åŠ¿ã€æˆæœ¬åˆ†è§£ï¼‰
        * scatter - æ•£ç‚¹å›¾ï¼ˆç›¸å…³æ€§ã€åˆ†å¸ƒï¼‰
        * bubble - æ°”æ³¡å›¾ï¼ˆä¸‰ç»´å¯¹æ¯”ã€ä½“é‡å·®å¼‚ï¼‰
        * stacked_bar - å †ç§¯æŸ±çŠ¶å›¾ï¼ˆç»“æ„åˆ†æã€å±‚çº§åˆ†è§£ï¼‰
        * heatmap - çƒ­åŠ›å›¾ï¼ˆçŸ©é˜µã€å¯†åº¦ã€äº§ä¸šé“¾ï¼‰
        * mixed - æ··åˆå›¾ï¼ˆå¤šæŒ‡æ ‡ã€å¤åˆåˆ†æï¼‰
      - ã€ç¦æ­¢ã€‘ä¸è¦åœ¨åŒä¸€ä»½æŠ¥å‘Šä¸­é‡å¤ä½¿ç”¨ç›¸åŒå›¾è¡¨ç±»å‹
    
    ========== ğŸ¯ å…·ä½“å†™ä½œè¦æ±‚ ==========
    
    1. ã€ç¯‡å¹…ä¸è´¨é‡ã€‘
       - ç›®æ ‡å­—æ•°ï¼š1000-1500 å­—
       - æ®µè½æ•°ï¼š5-8 æ®µ
       - æ¯æ®µé•¿åº¦ï¼š150-300 å­—
       - æ¯æ®µå‰æœ‰é€»è¾‘èµ·ç‚¹ï¼ˆä¸»é¢˜å¥ï¼‰
    
    2. ã€æ•°æ®ä¸æ¡ˆä¾‹ã€‘
       - è‡³å°‘åŒ…å« 5-10 ä¸ªæ•°æ®ç‚¹ï¼ˆå…·ä½“æ•°å­—ï¼‰
       - è‡³å°‘åŒ…å« 2-3 ä¸ªçœŸå®æ¡ˆä¾‹æˆ–å¯¹æ ‡åˆ†æ
       - æ•°æ®å¯†åº¦ç›®æ ‡ï¼šæ¯300å­—è‡³å°‘1ä¸ªæ•°æ®ç‚¹
    
    3. ã€è¡¨æ ¼ä¸å›¾è¡¨ - å¤šæ ·åŒ–ã€‘
       - å¿…é¡»åŒ…å«ï¼š1ä¸ª Markdown è¡¨æ ¼ï¼ˆ4è¡Œä»¥ä¸Šï¼‰
       - å¿…é¡»åŒ…å«ï¼š1ä¸ª JSON å›¾è¡¨å—ï¼ˆé€‰æ‹©æœ€åˆé€‚çš„å›¾è¡¨ç±»å‹ï¼‰
       - ã€å…³é”®ã€‘æ¯ä¸ªå­èŠ‚é€‰æ‹©ä¸åŒçš„å›¾è¡¨ç±»å‹ï¼Œä¿æŒè§†è§‰å¤šæ ·æ€§
       - å¿…é¡»åŒ…å«ï¼š1ä¸ª JSON å›¾è¡¨æ•°æ®å—ï¼ˆå®Œæ•´çš„ chart_type/data ç»“æ„ï¼‰
       - è¡¨æ ¼å’Œå›¾è¡¨å‰éœ€æœ‰è¿‡æ¸¡æ–‡å­—
    
    4. ã€æœ¯è¯­ä¸è¡¨è¿°ã€‘
       - ä¸“ä¸šæœ¯è¯­å æ¯”ï¼šæ¯100å­—è‡³å°‘1ä¸ªè¡Œä¸šæœ¯è¯­
       - é¿å…æ¨¡ç³Šè¡¨è¿°ï¼šç”¨å…·ä½“é‡è¯ä»£æ›¿"å¾ˆ"ã€"æ¯”è¾ƒ"ã€"å¤§å¹…"
       - ç¤ºä¾‹ï¼š
         âŒ "æˆæœ¬æ¯”è¾ƒé«˜"
         âœ… "å•ä½æˆæœ¬ä¸º Â¥X/ä»¶ï¼Œç›¸æ¯”ç«å“é«˜ 23%"
    
    5. ã€é€»è¾‘ä¸è®ºè¯ã€‘
       - ä½¿ç”¨é€»è¾‘è¯å¼ºåŒ–è®ºè¯ï¼šå› æ­¤ã€ç›¸æ¯”ã€åŸºäºã€è¯æ˜ã€è¡¨æ˜ã€è¿›è€Œã€ç”±æ­¤å¯è§ç­‰
       - é¿å…ç”Ÿç¡¬çš„åˆ—ä¸¾ï¼Œåº”è¯¥å½¢æˆå› æœé“¾
       - æ¯ä¸ªè®ºç‚¹åè·Ÿè¯æ®ï¼ˆå¦‚æœæœªæä¾›è¯æ®ï¼Œåˆ™è¯´æ˜éœ€è¦è¡¥å……ï¼‰
    
    ========== ğŸ“‹ JSON å›¾è¡¨ç¤ºä¾‹ ==========
    
    ```json
    {{
      "chart_type": "line",
      "title": "2020-2024å¹´è¡Œä¸šå¸‚åœºè§„æ¨¡ä¸å¢é•¿ç‡",
      "data": {{
        "labels": ["2020", "2021", "2022", "2023", "2024E"],
        "datasets": [
          {{"label": "å¸‚åœºè§„æ¨¡(Â¥äº¿)", "values": [100, 130, 169, 220, 286]}},
          {{"label": "YoYå¢é•¿ç‡(%)", "values": [15, 30, 30, 30, 30]}}
        ]
      }},
      "x_label": "å¹´ä»½",
      "y_label": "è§„æ¨¡/å¢é•¿ç‡"
    }}
    ```
    
    ========== ğŸ“Š æ”¯æŒçš„å›¾è¡¨ç±»å‹ï¼ˆ9ç§ï¼‰ ==========
    
    1. **bar** - æŸ±çŠ¶å›¾ï¼ˆå¯¹æ¯”ã€æ’åºï¼‰
    2. **line** - æŠ˜çº¿å›¾ï¼ˆè¶‹åŠ¿ã€æ—¶åºï¼‰
    3. **pie** - é¥¼å›¾ï¼ˆå æ¯”ã€åˆ†å¸ƒï¼‰
    4. **radar** - é›·è¾¾å›¾ï¼ˆå¤šç»´è¯„ä»·ã€å¯¹æ ‡ï¼‰
    5. **area** - é¢ç§¯å›¾ï¼ˆå †ç§¯ã€è¶‹åŠ¿ï¼‰
    6. **scatter** - æ•£ç‚¹å›¾ï¼ˆç›¸å…³æ€§ã€ç¦»æ•£åº¦ï¼‰
    7. **bubble** - æ°”æ³¡å›¾ï¼ˆä¸‰ç»´å¯¹æ¯”ï¼‰
    8. **stacked_bar** - å †ç§¯æŸ±çŠ¶å›¾ï¼ˆç»“æ„åˆ†æï¼‰
    9. **heatmap** - çƒ­åŠ›å›¾ï¼ˆçŸ©é˜µã€å¯†åº¦ï¼‰
    10. **mixed** - æ··åˆå›¾ï¼ˆæŸ±+æŠ˜çº¿ï¼‰
    
    ========== ğŸ’¡ é€‰å‹å»ºè®® ==========
    - æ•°æ®å¯¹æ ‡ã€å¸‚åœºæ’å â†’ æŸ±çŠ¶å›¾ (bar)
    - å¸‚åœºè¶‹åŠ¿ã€å¢é•¿ç‡ â†’ æŠ˜çº¿å›¾ (line)
    - å¸‚åœºä»½é¢ã€å æ¯” â†’ é¥¼å›¾ (pie)
    - æŠ€æœ¯è¯„åˆ†ã€å¤šç»´è¯„ä»· â†’ é›·è¾¾å›¾ (radar)
    - æˆæœ¬æ„æˆã€å±‚çº§åˆ†è§£ â†’ å †ç§¯æŸ±çŠ¶å›¾ (stacked_bar)
    - äº§ä¸šé“¾åˆ†æã€æ€§èƒ½å¯¹æ¯” â†’ çƒ­åŠ›å›¾ (heatmap)
    - ç›¸å…³æ€§åˆ†æã€æ•£å¸ƒæƒ…å†µ â†’ æ•£ç‚¹å›¾ (scatter)
    - å¿«é€Ÿå¢é•¿ã€ä½“é‡å¯¹æ¯” â†’ æ°”æ³¡å›¾ (bubble)
    - å¤šä¸ªæŒ‡æ ‡æ··åˆ â†’ æ··åˆå›¾ (mixed)
    - é¢ç§¯å †ç§¯åˆ†æ â†’ é¢ç§¯å›¾ (area)
    
    ========== ğŸ” æœ€åæ£€æŸ¥æ¸…å• ==========
    åœ¨æäº¤å†…å®¹å‰ï¼Œè‡ªæ£€ï¼š
    â–¡ ç¯‡å¹… >= 1000 å­—ï¼Ÿ
    â–¡ æœ‰3-4å±‚é€»è¾‘ç»“æ„ï¼ˆ###/#### æ ‡é¢˜ï¼‰ï¼Ÿ
    â–¡ æ•°æ®å¯†åº¦ >= 3%ï¼Ÿ
    â–¡ è‡³å°‘5ä¸ªæ•°æ®ç‚¹ï¼Œæ ‡æ³¨äº†æ¥æºï¼Ÿ
    â–¡ åŒ…å«1ä¸ªè¡¨æ ¼ï¼ˆ4è¡Œä»¥ä¸Šï¼‰ï¼Ÿ
    â–¡ åŒ…å«1ä¸ªå®Œæ•´çš„JSONå›¾è¡¨å—ï¼Ÿ
    â–¡ ä½¿ç”¨äº†5ä¸ªä»¥ä¸Šé€»è¾‘è¯ï¼Ÿ
    â–¡ ä¸“ä¸šæœ¯è¯­è‡ªç„¶èå…¥ï¼Ÿ
    â–¡ æ®µè½é—´é€»è¾‘é€’è¿›æ¸…æ™°ï¼Ÿ
    
    ========== å®é™…å†™ä½œå†…å®¹ ==========
    {base_prompt}
    """
    return enhanced

def create_chart_from_description_plotly(chart_data, output_path):
    """
    ã€ä¼ä¸šçº§å›¾è¡¨ v2.0ã€‘ä½¿ç”¨ Plotly ç”Ÿæˆäº¤äº’å¼ä¸“ä¸šå›¾è¡¨
    æ”¯æŒ: Bar, Line, Pie, Radar, Mixed
    è¾“å‡º: HTMLï¼ˆå¯äº¤äº’ï¼‰å’Œ PNGï¼ˆé™æ€ï¼‰
    """
    try:
        import plotly.graph_objects as go
        import plotly.io as pio
        
        # æ•°æ®éªŒè¯
        if not chart_data or not isinstance(chart_data, dict):
            return False
        
        c_type = chart_data.get('chart_type', 'bar').lower()
        title = chart_data.get('title', 'æ•°æ®åˆ†æ')
        data = chart_data.get('data', {})
        labels = data.get('labels', [])
        datasets = data.get('datasets', [])
        x_label = chart_data.get('x_label', '')
        y_label = chart_data.get('y_label', '')
        
        # åŸºç¡€éªŒè¯
        if not labels or not datasets:
            return False
        
        fig = go.Figure()
        
        # ========== æŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€æ··åˆå›¾ ==========
        if c_type in ['bar', 'line', 'mixed']:
            for idx, ds in enumerate(datasets):
                if not isinstance(ds, dict):
                    continue
                label = ds.get('label', f'Series {idx+1}')
                vals = ds.get('values', [])
                
                if not vals or len(vals) != len(labels):
                    continue
                
                if c_type == 'line' or (c_type == 'mixed' and idx > 0):
                    fig.add_trace(go.Scatter(
                        x=labels, y=vals, mode='lines+markers',
                        name=label, line=dict(width=3),
                        hovertemplate='<b>%{x}</b><br>' + label + ': %{y}<extra></extra>'
                    ))
                else:
                    fig.add_trace(go.Bar(
                        x=labels, y=vals, name=label,
                        hovertemplate='<b>%{x}</b><br>' + label + ': %{y}<extra></extra>'
                    ))
        
        # ========== é¥¼å›¾ ==========
        elif c_type == 'pie':
            if datasets and isinstance(datasets[0], dict) and 'values' in datasets[0]:
                values = datasets[0]['values']
                if values and len(values) == len(labels):
                    fig = go.Figure(data=[go.Pie(
                        labels=labels, values=values,
                        hovertemplate='<b>%{label}</b><br>å æ¯”: %{value}<extra></extra>'
                    )])
                else:
                    return False
            else:
                return False
        
        # ========== é›·è¾¾å›¾ ==========
        elif c_type == 'radar':
            for ds in datasets:
                if not isinstance(ds, dict):
                    continue
                label = ds.get('label', '')
                vals = ds.get('values', [])
                if vals and len(vals) == len(labels):
                    fig.add_trace(go.Scatterpolar(
                        r=vals, theta=labels, fill='toself',
                        name=label,
                        hovertemplate='<b>%{theta}</b><br>' + label + ': %{r}<extra></extra>'
                    ))
        
        # ========== å¸ƒå±€ä¼˜åŒ– ==========
        fig.update_layout(
            title={'text': title, 'font': {'size': 18, 'color': '#1f77b4'}},
            xaxis_title=x_label,
            yaxis_title=y_label,
            hovermode='x unified',
            template='plotly_white',
            font=dict(family="Arial, sans-serif", size=12),
            plot_bgcolor='rgba(240, 240, 240, 0.5)',
            paper_bgcolor='white',
            width=1000,
            height=600,
            showlegend=True,
            legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
        )
        
        # ========== ä¿å­˜è¾“å‡º ==========
        html_path = output_path.replace('.png', '.html')
        fig.write_html(html_path)
        print(f"      ğŸ–¼ï¸ Plotly HTML å·²ç”Ÿæˆ: {html_path}")
        
        # åŒæ—¶ä¿å­˜PNG (é™æ€ç‰ˆæœ¬) - å¯é€‰
        try:
            pio.write_image(fig, output_path, width=1000, height=600)
            print(f"      ğŸ“Š Plotly PNG å·²ç”Ÿæˆ: {output_path}")
            return True
        except Exception as png_err:
            # Kaleidoå¯èƒ½æœªå®‰è£…ï¼Œåªä¿å­˜HTMLä¹Ÿå¯ä»¥
            print(f"      â„¹ï¸ PNG è¾“å‡ºéœ€è¦ kaleido åº“ (å·²ä¿å­˜ HTML ç‰ˆæœ¬)")
            return True
            
    except ImportError as ie:
        print(f"      â„¹ï¸ Plotly ä¸å¯ç”¨: {ie}")
        return False
    except Exception as e:
        print(f"      âš ï¸ Plotly å¤„ç†å¼‚å¸¸: {type(e).__name__}: {str(e)[:80]}")
        return False

def create_chart_from_description(chart_desc, output_path):
    """
    ã€é‡æ„ç‰ˆã€‘ä» JSON æ•°æ®ç”Ÿæˆå•†ä¸šçº§ (Business-Level) å›¾è¡¨
    æ”¯æŒç±»å‹: Bar, Line, Pie, Radar, Mixed
    é£æ ¼: å•†åŠ¡è“è°ƒ, é«˜åˆ†è¾¨ç‡, ä¸­æ–‡æ”¯æŒ
    ã€æ–°å¢ã€‘æ•°æ®éªŒè¯å’Œç©ºç™½å›¾è¡¨æ£€æµ‹
    """
    try:
        # 1. æå– JSON æ•°æ®
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', chart_desc, re.DOTALL)
        if not json_match:
            # å°è¯•ç›´æ¥æ‰¾ {}
            json_match = re.search(r'(\{.*"chart_type".*\})', chart_desc, re.DOTALL)
            
        if not json_match:
            print("      â„¹ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆ JSON å›¾è¡¨æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
            return False
            
        data_str = json_match.group(1)
        try:
            chart_data = json.loads(data_str)
        except json.JSONDecodeError as je:
            print(f"      âš ï¸ JSON è§£æå¤±è´¥: {str(je)[:80]}")
            return False

        # ã€æ–°å¢ã€‘æ•°æ®éªŒè¯
        labels = chart_data.get('data', {}).get('labels', [])
        datasets = chart_data.get('data', {}).get('datasets', [])
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºç™½æˆ–æ— æ•ˆæ•°æ®
        if not labels or not datasets:
            print("      âš ï¸ å›¾è¡¨æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ç»˜å›¾")
            return False
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        for ds in datasets:
            if not ds.get('values') or len(ds['values']) != len(labels):
                print(f"      âš ï¸ æ•°æ®ä¸ä¸€è‡´ï¼švaluesé•¿åº¦ {len(ds.get('values', []))} != labelsé•¿åº¦ {len(labels)}")
                return False
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å…¨ä¸º0æˆ–æ— æ•ˆ
        all_values = []
        for ds in datasets:
            all_values.extend(ds.get('values', []))
        if not all_values or all(v == 0 or v is None for v in all_values):
            print("      âš ï¸ æ‰€æœ‰æ•°æ®ç‚¹å‡ä¸º0æˆ–ç©ºï¼Œç”Ÿæˆå¤‡é€‰è¡¨æ ¼")
            return False

        # ========== ä¼˜å…ˆå°è¯• Plotlyï¼ˆäº¤äº’å¼ï¼‰ ==========
        if create_chart_from_description_plotly(chart_data, output_path):
            return True
        
        # ========== é™çº§åˆ° Matplotlibï¼ˆé™æ€ï¼‰ ==========
        print("      ğŸ“Š ä½¿ç”¨ Matplotlib ç”Ÿæˆé™æ€å›¾è¡¨...")
        
        # 2. å‡†å¤‡ç»˜å›¾ä¸Šä¸‹æ–‡
        plt.clf()
        plt.close('all')
        
        # è®¾ç½®å•†ä¸šé£æ ¼ (Business Style)
        # èƒŒæ™¯è‰²: æµ…ç°/ç™½, ç½‘æ ¼: ç°è‰²è™šçº¿, å­—ä½“è‰²: æ·±ç°
        plt.rcParams.update({
            'figure.facecolor': '#FFFFFF',
            'axes.facecolor': '#F8F9FA',
            'axes.edgecolor': '#DEE2E6',
            'axes.grid': True,
            'grid.color': '#E9ECEF',
            'grid.linestyle': '--',
            'grid.alpha': 0.8,
            'text.color': '#343A40',
            'axes.labelcolor': '#495057',
            'xtick.color': '#495057',
            'ytick.color': '#495057',
            'font.size': 10
        })
        
        # å•†ä¸šé…è‰²æ–¹æ¡ˆ (æ·±è“, ç§‘æŠ€è“, æ´»åŠ›æ©™, ç¨³é‡ç°)
        COLORS = ['#0056B3', '#20C997', '#FD7E14', '#6C757D', '#6610F2', '#E83E8C']
        
        fig, ax = plt.subplots(figsize=(10, 6), dpi=150) # æé«˜åˆ†è¾¨ç‡
        
        # å­—ä½“åº”ç”¨ (ç¡®ä¿ä¸­æ–‡)
        font_prop = CHINESE_FONT if CHINESE_FONT else None
        
        # 3. è§£ææ•°æ®
        c_type = chart_data.get('chart_type', 'bar').lower()
        title = chart_data.get('title', 'æ•°æ®åˆ†æ')
        x_label = chart_data.get('x_label', '')
        y_label = chart_data.get('y_label', '')

        # 4. ç»˜åˆ¶é€»è¾‘ - æ”¯æŒ8+ç§å›¾è¡¨ç±»å‹
        if c_type == 'pie':
            # ã€é¥¼å›¾ã€‘- å æ¯”åˆ†æ
            if not datasets or 'values' not in datasets[0]:
                return False
            values = datasets[0]['values']
            pie_labels = labels
            textprops_dict = {}
            if font_prop:
                textprops_dict['fontproperties'] = font_prop
            wedges, texts, autotexts = ax.pie(
                values, labels=pie_labels, autopct='%1.1f%%',
                startangle=90, colors=COLORS,
                textprops=textprops_dict
            )
            for text in autotexts: 
                text.set_color('white')

        elif c_type == 'radar':
            # ã€é›·è¾¾å›¾ v2.0ã€‘- å¤šç»´åº¦å¯¹æ¯” (å·²ä¿®å¤æ˜¾ç¤ºé—®é¢˜)
            plt.close(fig)
            fig = plt.figure(figsize=(10, 8), dpi=150)
            ax = fig.add_subplot(111, polar=True)
            
            # ä½¿ç”¨ Python å†…ç½®æ›¿ä»£ numpy
            import math
            num_vars = len(labels)
            angles = [2 * math.pi * i / num_vars for i in range(num_vars)]
            angles += angles[:1]  # é—­åˆ
            
            # è®¾ç½®é›·è¾¾å›¾æ ‡ç­¾
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(labels, fontproperties=font_prop, size=10)
            ax.set_ylim(0, 100)  # è®¾ç½®åŠå¾„èŒƒå›´
            ax.set_rlabel_position(0)  # æ ‡ç­¾ä½ç½®
            
            # ç»˜åˆ¶æ¯ä¸ªæ•°æ®é›†
            for idx, ds in enumerate(datasets):
                vals = ds.get('values', [])
                if len(vals) != num_vars:
                    continue
                vals_plot = vals + vals[:1]  # é—­åˆ
                ax.plot(angles, vals_plot, 'o-', linewidth=2.5, label=ds.get('label', f'æ•°æ®{idx+1}'),
                        color=COLORS[idx % len(COLORS)])
                ax.fill(angles, vals_plot, alpha=0.25, color=COLORS[idx % len(COLORS)])
            
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)
            ax.grid(True, linestyle='--', alpha=0.7)

        elif c_type == 'area':
            # ã€é¢ç§¯å›¾ã€‘- è¶‹åŠ¿å †ç§¯åˆ†æ
            x = range(len(labels))
            bottom = None
            for idx, ds in enumerate(datasets):
                vals = ds.get('values', [])
                if len(vals) == len(labels):
                    ax.fill_between(x, 0 if bottom is None else bottom, 
                                   [v + (0 if bottom is None else b) for v, b in zip(vals, bottom or [0]*len(vals))],
                                   label=ds.get('label', f'Series {idx+1}'),
                                   color=COLORS[idx % len(COLORS)], alpha=0.7)
                    bottom = [v + (b or 0) for v, b in zip(vals, bottom or [0]*len(vals))]
            
            ax.set_xticks(x)
            ax.set_xticklabels(labels, fontproperties=font_prop)
            if x_label: ax.set_xlabel(x_label, fontproperties=font_prop)
            if y_label: ax.set_ylabel(y_label, fontproperties=font_prop)
            ax.legend(prop=font_prop)

        elif c_type == 'scatter':
            # ã€æ•£ç‚¹å›¾ã€‘- ç›¸å…³æ€§åˆ†æ
            for idx, ds in enumerate(datasets):
                vals = ds.get('values', [])
                if len(vals) == len(labels):
                    x_vals = range(len(labels))
                    ax.scatter(x_vals, vals, s=100, alpha=0.6, 
                              label=ds.get('label', f'Series {idx+1}'),
                              color=COLORS[idx % len(COLORS)])
            
            ax.set_xticks(range(len(labels)))
            ax.set_xticklabels(labels, fontproperties=font_prop)
            if x_label: ax.set_xlabel(x_label, fontproperties=font_prop)
            if y_label: ax.set_ylabel(y_label, fontproperties=font_prop)
            ax.legend(prop=font_prop)

        elif c_type == 'stacked_bar':
            # ã€å †ç§¯æŸ±çŠ¶å›¾ã€‘- ç»„æˆç»“æ„åˆ†æ
            x = range(len(labels))
            bottom = [0] * len(labels)
            for idx, ds in enumerate(datasets):
                vals = ds.get('values', [])
                if len(vals) == len(labels):
                    ax.bar(x, vals, bottom=bottom, label=ds.get('label', f'Series {idx+1}'),
                          color=COLORS[idx % len(COLORS)], alpha=0.9)
                    bottom = [b + v for b, v in zip(bottom, vals)]
            
            ax.set_xticks(x)
            ax.set_xticklabels(labels, fontproperties=font_prop)
            if x_label: ax.set_xlabel(x_label, fontproperties=font_prop)
            if y_label: ax.set_ylabel(y_label, fontproperties=font_prop)
            ax.legend(prop=font_prop)

        elif c_type == 'bubble':
            # ã€æ°”æ³¡å›¾ã€‘- ä¸‰ç»´æ•°æ®å¯¹æ¯”
            for idx, ds in enumerate(datasets):
                vals = ds.get('values', [])
                if len(vals) == len(labels):
                    x_vals = range(len(labels))
                    sizes = [abs(v) * 10 + 50 for v in vals]  # æ°”æ³¡å¤§å°
                    ax.scatter(x_vals, vals, s=sizes, alpha=0.5,
                              label=ds.get('label', f'Series {idx+1}'),
                              color=COLORS[idx % len(COLORS)])
            
            ax.set_xticks(range(len(labels)))
            ax.set_xticklabels(labels, fontproperties=font_prop)
            if x_label: ax.set_xlabel(x_label, fontproperties=font_prop)
            if y_label: ax.set_ylabel(y_label, fontproperties=font_prop)
            ax.legend(prop=font_prop)

        elif c_type == 'heatmap':
            # ã€çƒ­åŠ›å›¾ã€‘- çŸ©é˜µæ•°æ®åˆ†æï¼ˆéœ€è¦ numpy åº“æ”¯æŒï¼‰
            try:
                import numpy as np
                data_matrix = []
                for ds in datasets:
                    vals = ds.get('values', [])
                    if len(vals) == len(labels):
                        data_matrix.append(vals)
                
                if data_matrix:
                    data_matrix = np.array(data_matrix)
                    im = ax.imshow(data_matrix, cmap='RdYlBu_r', aspect='auto')
                    ax.set_xticks(range(len(labels)))
                    ax.set_yticks(range(len(datasets)))
                    ax.set_xticklabels(labels, fontproperties=font_prop)
                    ax.set_yticklabels([ds.get('label', f'Row {i}') for i, ds in enumerate(datasets)], 
                                      fontproperties=font_prop)
                    # æ·»åŠ é¢œè‰²æ¡
                    cbar = plt.colorbar(im, ax=ax)
                    cbar.set_label(y_label or 'æ•°å€¼', fontproperties=font_prop)
            except ImportError:
                # numpy ä¸å¯ç”¨ï¼Œæ˜¾ç¤ºæ–‡æœ¬æç¤º
                print(f"      â„¹ï¸ çƒ­åŠ›å›¾éœ€è¦ numpy åº“æ”¯æŒï¼Œå·²è·³è¿‡æ­¤å›¾è¡¨ç±»å‹")
                ax.text(0.5, 0.5, 'Heatmap å›¾è¡¨ç±»å‹\néœ€è¦ numpy åº“æ”¯æŒ', 
                       ha='center', va='center', transform=ax.transAxes, 
                       fontproperties=font_prop, color='gray', fontsize=12)
                ax.set_xticks([])
                ax.set_yticks([])

        else:
            # Bar / Line / Mixed (é»˜è®¤)
            x = range(len(labels))
            width = 0.35
            
            for idx, ds in enumerate(datasets):
                label = ds.get('label', f'Series {idx+1}')
                vals = ds.get('values', [])
                
                if len(vals) != len(labels): 
                    continue
                
                current_type = c_type
                if c_type == 'mixed':
                    current_type = 'bar' if idx == 0 else 'line'
                
                if current_type == 'line':
                    ax.plot(labels, vals, marker='o', linewidth=2.5,
                            color=COLORS[idx % len(COLORS)], label=label, markersize=8)
                else:
                    offset = (idx - len(datasets)/2) * width + width/2
                    if c_type == 'mixed': offset = 0
                    
                    rects = ax.bar([i + offset for i in x], vals, width,
                           label=label, color=COLORS[idx % len(COLORS)], alpha=0.9)
                    ax.bar_label(rects, padding=3, fmt='%.1f', fontsize=8)

            ax.set_xticks(x)
            ax.set_xticklabels(labels, fontproperties=font_prop)
            if x_label: ax.set_xlabel(x_label, fontproperties=font_prop)
            if y_label: ax.set_ylabel(y_label, fontproperties=font_prop)
            ax.legend(prop=font_prop)

        # 5. é€šç”¨ä¿®é¥°
        ax.set_title(title, fontproperties=font_prop, fontsize=16, fontweight='bold', pad=25, color='#212529')
        
        # ç§»é™¤é¡¶éƒ¨å’Œå³ä¾§è¾¹æ¡† (Clean Look)
        if c_type != 'heatmap':
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(output_path, format='png', bbox_inches='tight', dpi=150)
        plt.close()
        
        print(f"      âœ… å›¾è¡¨å·²ç”Ÿæˆ: {output_path}")
        return True
    except Exception as e:
        print(f"      âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        return False


def embed_chart_in_markdown(content, section_title, chapter_title, output_dir):
    """
    æ£€æµ‹å†…å®¹ä¸­çš„å›¾è¡¨æè¿°ï¼Œç”Ÿæˆå®é™…å›¾è¡¨å¹¶åµŒå…¥Markdown
    ä¼šè‡ªåŠ¨å°†ç”Ÿæˆå›¾è¡¨çš„ä»£ç å— **æ›¿æ¢** ä¸ºå›¾ç‰‡ï¼Œé¿å…æ–‡ä¸­æ®‹ç•™ä»£ç 
    ã€æ–°å¢ã€‘å¦‚æœå›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œè‡ªåŠ¨é™çº§ä¸ºè¡¨æ ¼
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰å›¾è¡¨æè¿°æ ‡è®°
    if '```' not in content and '|' not in content:
        return content
    
    safe_title = re.sub(r'[^\w\-]', '_', section_title)[:20]
    charts_dir = os.path.join(output_dir, "charts")
    
    try:
        os.makedirs(charts_dir, exist_ok=True)
        
        # ä½¿ç”¨æ­£åˆ™æŸ¥æ‰¾æ‰€æœ‰ä»£ç å— (åŒ…æ‹¬ python å’Œ çº¯æ–‡æœ¬)
        # æ•è·ç»„ 1: å®Œæ•´çš„ä»£ç å— (ç”¨äºæ›¿æ¢)
        # æ•è·ç»„ 2: ä»£ç å—å†…éƒ¨çš„å†…å®¹ (ç”¨äºè§£æ)
        pattern = re.compile(r'(```(?:python|py|json)?\s*(\n.*?)```)', re.IGNORECASE | re.DOTALL)
        
        matches = pattern.findall(content)
        
        for i, (full_block, inner_code) in enumerate(matches):
            # åˆ¤æ–­æ˜¯å¦æ˜¯å›¾è¡¨ç›¸å…³çš„ä»£ç å—
            is_chart_code = "matplotlib" in inner_code or "plt." in inner_code
            is_chart_data = "å›¾è¡¨" in inner_code and "|" in inner_code
            is_json_chart = '"chart_type"' in inner_code and '"data"' in inner_code
            
            if is_chart_code or is_chart_data or is_json_chart:
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                unique_id = f"{int(time.time())}_{i}"
                filename = f"{safe_title}_{unique_id}.png"
                chart_abs_path = os.path.join(charts_dir, filename)
                chart_rel_path = f"./charts/{filename}"
                
                # å°è¯•ç”Ÿæˆå›¾è¡¨
                success = create_chart_from_description(inner_code, chart_abs_path)
                
                if success and os.path.exists(chart_abs_path):
                    # å›¾è¡¨ç”ŸæˆæˆåŠŸ
                    img_tag = f"\n\n![{section_title}æ•°æ®åˆ†æ]({chart_rel_path})\n\n"
                    content = content.replace(full_block, img_tag)
                    print(f"      âœ… å·²ç”Ÿæˆå›¾è¡¨: {filename}")
                else:
                    # å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œå°è¯•ä»JSONä¸­æå–è¡¨æ ¼æ•°æ®ä½œä¸ºå¤‡é€‰
                    print(f"      âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œå°è¯•å¤‡é€‰è¡¨æ ¼æ–¹æ¡ˆ...")
                    
                    # ä» JSON ä¸­æå–æ•°æ®ç”Ÿæˆè¡¨æ ¼
                    json_match = re.search(r'(\{.*"chart_type".*\})', inner_code, re.DOTALL)
                    if json_match:
                        try:
                            chart_data = json.loads(json_match.group(1))
                            fallback_table = _generate_fallback_table(chart_data)
                            if fallback_table:
                                content = content.replace(full_block, f"\n\n{fallback_table}\n\n")
                                print(f"      âœ… å·²ç”Ÿæˆå¤‡é€‰è¡¨æ ¼")
                                continue
                        except:
                            pass
                    
                    # å¦‚æœéƒ½å¤±è´¥ï¼Œä¿ç•™åŸä»£ç å—ä½†æ ‡è®°ä¸ºå¾…å¤„ç†
                    marked_block = f"\n\nâš ï¸ [å›¾è¡¨ç”Ÿæˆå¤±è´¥ - è¯·æ‰‹åŠ¨å¤„ç†]\n{full_block}\n\n"
                    content = content.replace(full_block, marked_block)
                    print(f"      âš ï¸ å·²æ ‡è®°ä¸ºå¾…å¤„ç†")
                
    except Exception as e:
        print(f"      âš ï¸ åµŒå…¥å›¾è¡¨å¤±è´¥: {e}")
    
    return content


def _generate_fallback_table(chart_data):
    """
    ä»å›¾è¡¨JSONæ•°æ®ç”ŸæˆMarkdownè¡¨æ ¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
    """
    try:
        labels = chart_data.get('data', {}).get('labels', [])
        datasets = chart_data.get('data', {}).get('datasets', [])
        title = chart_data.get('title', 'æ•°æ®è¡¨')
        
        if not labels or not datasets:
            return None
        
        # æ„å»ºè¡¨æ ¼
        header = '| ' + ' | '.join(['æŒ‡æ ‡'] + labels) + ' |'
        separator = '| ' + ' | '.join(['---'] * (len(labels) + 1)) + ' |'
        
        rows = []
        for ds in datasets:
            label = ds.get('label', 'æ•°æ®')
            values = ds.get('values', [])
            if len(values) == len(labels):
                row = '| ' + ' | '.join([label] + [str(v) for v in values]) + ' |'
                rows.append(row)
        
        if rows:
            table = f"\n\n**è¡¨: {title}**\n\n{header}\n{separator}\n" + "\n".join(rows) + "\n\n"
            return table
    except:
        pass
    
    return None


# ================== ğŸ“š å¢å¼ºçš„ RAG ä¸ä¸Šä¸‹æ–‡ç®¡ç† ==================

class ContextManager:
    """ç®¡ç†å·²ç”Ÿæˆå†…å®¹çš„ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒäº¤å‰å¼•ç”¨"""
    def __init__(self):
        self.generated_sections = {}  # {chapter_title: {section_title: content}}
        self.section_summaries = {}   # å¿«é€Ÿç´¢å¼•
        self.cache = {
            "outline": None,
            "style_guide": "",
            "section_plans": {},
            "global_thesis": "",
            "last_exec_summary": ""
        }
    
    def set_master_plan(self, outline, style_guide):
        """ç¼“å­˜ Master Planner ç»“æœï¼ˆå¤§çº² + é£æ ¼æŒ‡å—ï¼‰"""
        self.cache["outline"] = outline
        self.cache["style_guide"] = style_guide or ""
    
    def set_global_thesis(self, thesis):
        self.cache["global_thesis"] = thesis or ""
    
    def get_global_thesis(self):
        return self.cache.get("global_thesis", "")
    
    def set_last_exec_summary(self, summary):
        self.cache["last_exec_summary"] = summary or ""
    
    def get_last_exec_summary(self):
        return self.cache.get("last_exec_summary", "")

    def add_section_plan(self, section_key, plan_text):
        """ç¼“å­˜ Section Planner çš„æ‹†è§£è®¡åˆ’"""
        if plan_text:
            self.cache["section_plans"][section_key] = plan_text
    
    def get_style_guide(self):
        return self.cache.get("style_guide", "")
    
    def get_outline(self):
        return self.cache.get("outline")
    
    def add_section(self, chapter, section, content):
        """æ·»åŠ ç”Ÿæˆçš„ç« èŠ‚"""
        if chapter not in self.generated_sections:
            self.generated_sections[chapter] = {}
        self.generated_sections[chapter][section] = content
        
        # ç”Ÿæˆæ‘˜è¦ä¾¿äºå¿«é€Ÿå¼•ç”¨
        summary = content[:200] + "..." if len(content) > 200 else content
        self.section_summaries[f"{chapter}_{section}"] = summary
    
    def get_related_context(self, topic, max_sections=3):
        """è·å–ç›¸å…³çš„å·²ç”Ÿæˆå†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡"""
        if not self.generated_sections:
            return ""
        
        # ç®€å•çš„ç›¸å…³æ€§æ£€ç´¢
        related = []
        for chapter, sections in self.generated_sections.items():
            for section, content in sections.items():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                if any(word in content for word in topic.split()):
                    related.append(f"[{chapter} - {section}]\n{content[:300]}...\n")
        
        return "\n".join(related[:max_sections])
    
    def get_summary(self):
        """ç”Ÿæˆå·²ç”Ÿæˆå†…å®¹çš„æ‘˜è¦"""
        summary = "å·²ç”Ÿæˆç« èŠ‚ï¼š\n"
        for chapter, sections in self.generated_sections.items():
            summary += f"- {chapter}:\n"
            for section in sections.keys():
                summary += f"  * {section}\n"
        return summary

def get_model(task_type):
    """
    æè‡´çœé’±çš„æ¨¡å‹è·¯ç”±ï¼š
    - deep_thinking: å¤§çº²/æ¶¦è‰²/å†™ä»£ç  â†’ Gemini 3 Pro
    - logic_planning: æ‹†è§£ç« èŠ‚/ç®€å•å®¡æ ¸ â†’ Gemini 2.5 Flashï¼ˆå¯æŒ‰è´¨é‡åˆ‡æ¢ï¼‰
    - heavy_reading: æœç´¢ã€é˜…è¯»ç½‘é¡µã€æ•°æ®æ¸…æ´— â†’ Gemini 2.5 Flash Lite
    """
    router = {
        "deep_thinking": CONF.GEMINI_PRO_MODEL,
        "logic_planning": CONF.GEMINI_FLASH_MODEL,
        "heavy_reading": CONF.GEMINI_FLASH_LITE_MODEL
    }
    return router.get(task_type, CONF.GEMINI_PRO_MODEL)

def call_model(prompt, model_id, temperature=0.6, response_mime_type="text/plain"):
    """é€šç”¨ Gemini è°ƒç”¨å°è£…ï¼Œæ”¯æŒé€‰æ‹©æ¨¡å‹/æ¸©åº¦"""
    api_version = "v1beta"
    url = f"https://generativelanguage.googleapis.com/{api_version}/models/{model_id}:generateContent?key={CONF.GEMINI_API_KEY}"
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": temperature,
            "top_p": 0.95,
            "top_k": 40,
            "response_mime_type": response_mime_type
        }
    }
    resp = call_api_robust(url, payload, headers={"Content-Type": "application/json"}, proxies=CONF.PROXIES_CLOUD, timeout=120)
    if not resp:
        # å…è®¸ç›´è¿å…œåº•
        resp = call_api_robust(url, payload, headers={"Content-Type": "application/json"}, proxies=CONF.PROXIES_LOCAL, timeout=120)
    if not resp:
        return None
    try:
        return resp['candidates'][0]['content']['parts'][0]['text']
    except Exception:
        print("âŒ Gemini è¿”å›ç»“æ„å¼‚å¸¸")
        return None

def call_flash(prompt, temperature=0.6, task_type="logic_planning"):
    """Flash æ¨¡å‹ï¼ˆè“é¢†ï¼šæœç´¢/ç²—å†™/æ•°æ®æ•´ç†ï¼‰"""
    return call_model(prompt, get_model(task_type), temperature=temperature)

def call_pro(prompt, temperature=0.4, response_mime_type="text/plain"):
    """Pro æ¨¡å‹ï¼ˆç™½é¢†ï¼šè§„åˆ’/æ¶¦è‰²/å†™ä»£ç ï¼‰"""
    return call_model(prompt, get_model("deep_thinking"), temperature=temperature, response_mime_type=response_mime_type)

def call_gemini(prompt, json_mode=False):
    """è°ƒç”¨ Gemini (ç”¨äºå¤§çº²/ç»Ÿç­¹)ï¼Œå¸¦æ˜ç¡®çš„é”™è¯¯å¤„ç†
    ä½¿ç”¨ gemini-3-pro-preview è·å¾—æœ€ä¼˜çš„ç»“æ„åŒ–è¾“å‡º"""
    model_id = CONF.GEMINI_OUTLINE_MODEL
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_id}:generateContent?key={CONF.GEMINI_API_KEY}"
    payload = {
        "contents": [{"parts": [{"text": prompt}]}], 
        "generationConfig": {
            "response_mime_type": "application/json" if json_mode else "text/plain",
            "temperature": 0.3,  # å¤§çº²ç”Ÿæˆä½¿ç”¨æ›´ä½çš„æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            "top_p": 0.95,
            "top_k": 40
        }
    }
    
    # å…ˆèµ°ä»£ç†ï¼Œå¤±è´¥åç›´è¿é‡è¯•ä¸€æ¬¡ï¼›ä»£ç†ä¸å¯ç”¨æ—¶ç›´æ¥ç›´è¿
    resp = call_api_robust(url, payload, headers={"Content-Type": "application/json"}, proxies=CONF.PROXIES_CLOUD, timeout=120)
    if not resp:
        print("   â†» å°è¯•ç›´è¿ Gemini å†è¯•ä¸€æ¬¡...")
        resp = call_api_robust(url, payload, headers={"Content-Type": "application/json"}, proxies={"http": None, "https": None}, timeout=120)
    if not resp:
        return None
    try:
        return resp['candidates'][0]['content']['parts'][0]['text']
    except Exception:
        print("âŒ Gemini è¿”å›ç»“æ„å¼‚å¸¸")
        return None

def call_local(prompt, model_name=None, temperature=0.6):
    """
    ç›´æ¥è°ƒç”¨ Geminiï¼ˆæ­£æ–‡å†™ä½œï¼‰ï¼Œæ”¯æŒæ¸©åº¦å¯è°ƒã€‚
    temperature: 0.3-0.5(ç²¾å‡†) / 0.6(å‡è¡¡) / 0.7-0.9(åˆ›æ„)
    ä½¿ç”¨ gemini-3-pro-preview è·å¾—æœ€ä¼˜çš„å†…å®¹ç”Ÿæˆè´¨é‡
    """
    # é™„åŠ å›¾è¡¨ç±»å‹å‚è€ƒä¿¡æ¯
    chart_type_hint = """
    
    ã€å›¾è¡¨ç±»å‹é€ŸæŸ¥è¡¨ - æ ¹æ®å†…å®¹è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„å›¾è¡¨ã€‘
    
    1. bar (æŸ±çŠ¶å›¾) - åœºæ™¯ï¼šå¯¹æ¯”åˆ†æã€æ’åã€å¸‚åœºä»½é¢å¯¹æ ‡
       ç¤ºä¾‹ï¼šä¸åŒå‚å•†çš„å¸‚åœºä»½é¢å¯¹æ¯”ã€2024-2025ç«å“å¯¹æ ‡
    
    2. line (æŠ˜çº¿å›¾) - åœºæ™¯ï¼šè¶‹åŠ¿åˆ†æã€æ—¶åºå˜åŒ–ã€å¢é•¿ç‡
       ç¤ºä¾‹ï¼š2020-2024å¸‚åœºè§„æ¨¡å¢é•¿è¶‹åŠ¿ã€äº§èƒ½åˆ©ç”¨ç‡å˜åŒ–
    
    3. pie (é¥¼å›¾) - åœºæ™¯ï¼šå æ¯”åˆ†æã€å¸‚åœºæ„æˆã€æˆæœ¬åˆ†é…
       ç¤ºä¾‹ï¼šå¸‚åœºä»½é¢åˆ†å¸ƒï¼ˆAä¼ä¸š50%ã€Bä¼ä¸š30%...ï¼‰
    
    4. radar (é›·è¾¾å›¾) - åœºæ™¯ï¼šå¤šç»´è¯„ä»·ã€æŠ€æœ¯å¯¹æ ‡ã€èƒ½åŠ›è¯„åˆ†
       ç¤ºä¾‹ï¼šäº§å“æ€§èƒ½è¯„åˆ†ï¼ˆå¯é æ€§80ã€æ•ˆç‡90ã€æˆæœ¬70...ï¼‰
    
    5. area (é¢ç§¯å›¾) - åœºæ™¯ï¼šå †ç§¯è¶‹åŠ¿ã€æˆæœ¬åˆ†è§£ã€å±‚çº§åˆ†æ
       ç¤ºä¾‹ï¼šç”µåŠ›æˆæœ¬åˆ†å¸ƒï¼ˆç‡ƒç…¤60%ã€æ°´ç”µ30%ã€é£ç”µ10%ï¼‰çš„å˜åŒ–è¶‹åŠ¿
    
    6. scatter (æ•£ç‚¹å›¾) - åœºæ™¯ï¼šç›¸å…³æ€§åˆ†æã€åˆ†å¸ƒæ˜¾ç¤ºã€ç¦»æ•£åº¦
       ç¤ºä¾‹ï¼šäº§å“ä»·æ ¼ vs å¸‚åœºå æœ‰ç‡çš„åˆ†å¸ƒå…³ç³»
    
    7. bubble (æ°”æ³¡å›¾) - åœºæ™¯ï¼šä¸‰ç»´å¯¹æ¯”ã€ä¼ä¸šè§„æ¨¡ã€ä½“é‡å·®å¼‚
       ç¤ºä¾‹ï¼šä¸åŒä¼ä¸šçš„è¥æ”¶ã€åˆ©æ¶¦ã€å¸‚åœºä»½é¢ä¸‰ç»´å¯¹æ¯”
    
    8. stacked_bar (å †ç§¯æŸ±çŠ¶å›¾) - åœºæ™¯ï¼šç»“æ„åˆ†æã€å±‚çº§åˆ†è§£
       ç¤ºä¾‹ï¼šæ”¶å…¥ç»“æ„ï¼ˆä¸»è¥ä¸šåŠ¡ã€æŠ•èµ„æ”¶ç›Šã€å…¶ä»–ï¼‰çš„å¹´åº¦å¯¹æ¯”
    
    9. heatmap (çƒ­åŠ›å›¾) - åœºæ™¯ï¼šçŸ©é˜µåˆ†æã€äº§ä¸šé“¾ã€å¯†åº¦åˆ†å¸ƒ
       ç¤ºä¾‹ï¼šäº§ä¸šé“¾å„ç¯èŠ‚çš„ä»·å€¼è´¡çŒ®åº¦çŸ©é˜µ
    
    10. mixed (æ··åˆå›¾) - åœºæ™¯ï¼šå¤šæŒ‡æ ‡ã€å¤åˆåˆ†æã€é‡çº²ä¸åŒ
        ç¤ºä¾‹ï¼šé”€é‡ï¼ˆæŸ±çŠ¶ï¼‰+ å¢é•¿ç‡ï¼ˆæŠ˜çº¿ï¼‰çš„ç»„åˆå±•ç¤º
    
    ã€é€‰å‹åŸåˆ™ã€‘
    âœ“ æ¯ä¸ªå­èŠ‚æœ€å¤šç”¨1ä¸ªå›¾è¡¨
    âœ“ åŒä¸€ä»½æŠ¥å‘Šä¸­é¿å…é‡å¤ä½¿ç”¨ç›¸åŒå›¾è¡¨ç±»å‹
    âœ“ é€‰æ‹©æœ€èƒ½ä½“ç°æ•°æ®ç‰¹å¾çš„å›¾è¡¨ç±»å‹
    âœ“ ä¼˜å…ˆè€ƒè™‘ radar, area, heatmap ç­‰å¤æ‚ç±»å‹
    """
    
    model_id = model_name or CONF.GEMINI_GEN_MODEL
    # gemini-3-pro-preview éœ€è¦ v1beta API
    api_version = "v1beta"
    
    url = f"https://generativelanguage.googleapis.com/{api_version}/models/{model_id}:generateContent?key={CONF.GEMINI_API_KEY}"
    payload = {
        "contents": [{"parts": [{"text": prompt + chart_type_hint}]}],
        "generationConfig": {
            "temperature": temperature,
            "top_p": 0.95,  # å¢åŠ å¤šæ ·æ€§åŒæ—¶ä¿æŒç›¸å…³æ€§
            "top_k": 40
        }
    }
    resp = call_api_robust(url, payload, headers={"Content-Type": "application/json"}, proxies=CONF.PROXIES_CLOUD, timeout=120)
    if not resp:
        return None
    try:
        text = resp["candidates"][0]["content"]["parts"][0]["text"]
        return text.strip()
    except Exception as e:
        print(f"âŒ è§£æ Gemini å“åº”å¤±è´¥: {e}")
        return None

# ================== ğŸ§­ æœ€ä½³æ€§ä»·æ¯”æµæ°´çº¿ï¼ˆFlash vs Pro åˆ†å·¥ï¼‰ ==================

def extract_first_json_block(text):
    """ä»ä»»æ„æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ª JSON å¯¹è±¡"""
    if not text:
        return None
    try:
        return json.loads(text)
    except Exception:
        pass
    match = re.search(r'(\{[\s\S]*\})', text)
    if match:
        try:
            return json.loads(match.group(1))
        except Exception:
            return None
    return None

def extract_code_block(text, language="python"):
    """æå–æŒ‡å®šè¯­è¨€çš„ä»£ç å—"""
    if not text:
        return None
    pattern = rf"```{language}[\s\S]*?```"
    match = re.search(pattern, text, flags=re.IGNORECASE)
    if match:
        code = re.sub(rf"```{language}", "", match.group(0), flags=re.IGNORECASE)
        code = code.replace("```", "").strip()
        return code
    match = re.search(r"```[\s\S]*?```", text)
    if match:
        code = match.group(0).replace("```", "").strip()
        return code
    return None

def call_flash_json(prompt, temperature=0.25, task_type="heavy_reading"):
    """Flash JSON è¾“å‡ºï¼ˆç”¨äºæ•°æ®æ•´ç†ï¼‰"""
    return call_model(prompt, get_model(task_type), temperature=temperature, response_mime_type="application/json")

# ================== ğŸ›ï¸ å…¨å±€ä¸Šä¸‹æ–‡æ³¨å…¥ ==================

def get_key_constraints():
    """å›ºå®šçš„æ ¸å¿ƒçº¦æŸï¼Œä¾› Planner/Editor å‚è€ƒ"""
    return "\n".join([
        "- ä¸¥ç¦è™šæ„æ•°æ®ï¼Œæ•°å­—éœ€å¯è¿½æº¯",
        "- æ¯å°èŠ‚è‡³å°‘1è¡¨1å›¾ï¼Œå›¾è¡¨æ•°æ®éœ€è‡ªæ´½",
        f"- è´¨é‡è¯„åˆ†é˜ˆå€¼ >= {CONF.QUALITY_THRESHOLD}",
        "- è¯­æ°”ä¸“ä¸šã€å‡ç»ƒï¼Œä¸è¦è¥é”€è…”",
        "- Markdown è¾“å‡ºï¼Œä¸å†™ä»£ç /JSONï¼ˆé™¤å›¾è¡¨æ•°æ®ç”Ÿæˆé˜¶æ®µï¼‰"
    ])

def generate_global_thesis(topic, outline):
    """ç”Ÿæˆå…¨ä¹¦æ ¸å¿ƒä¸»æ—¨ï¼ˆGlobal Thesisï¼‰"""
    prompt = f"""
ä½ æ˜¯æ€»ç¼–è¾‘ï¼Œè¯·å‡ç»ƒã€Š{topic}ã€‹çš„å…¨ä¹¦æ ¸å¿ƒä¸»æ—¨ï¼ˆ80-120å­—ï¼‰ã€‚
å¯ç”¨å¤§çº²: {json.dumps(outline, ensure_ascii=False)[:1200]}
è¦æ±‚: ç”¨1æ®µè¯å†™å‡ºå…¨ä¹¦æ ¸å¿ƒè®ºç‚¹å’Œä»·å€¼ï¼Œä¸åˆ—ç‚¹ã€‚
"""
    thesis = call_pro(prompt, temperature=0.35)
    return thesis or ""

def build_executive_summary(topic, context_mgr, last_chapter_title, global_thesis):
    """
    åŒç»“æ„æ‘˜è¦ï¼š
    - Rolling Window: ä¸Šä¸€ç«  120-180 å­—
    - Global Thesis: 80-120 å­—
    """
    if not last_chapter_title or not context_mgr.generated_sections.get(last_chapter_title):
        return global_thesis or ""
    
    last_sections = context_mgr.generated_sections.get(last_chapter_title, {})
    last_text = "\n\n".join(list(last_sections.values()))
    
    prompt = f"""
ä½ æ˜¯ Executive Editorï¼Œç”Ÿæˆä¸‹ä¸€ç« çš„ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œåˆ†ä¸ºä¸¤æ®µï¼š
1) Rolling Windowï¼ˆä¸Šä¸€ç« å…³é”®å‘ç°ï¼Œ120-180å­—ï¼‰ï¼šå¼ºè°ƒæ•°æ®/ç»“è®ºï¼Œä¸ºåç»­ç« èŠ‚é“ºå«ï¼Œé¿å…é‡å¤ä¸Šä¸€ç« ç»“å°¾è¡¨è¿°ã€‚
2) Global Thesisï¼ˆ80-120å­—ï¼‰ï¼šé‡ç”³å…¨ä¹¦ä¸»æ—¨ï¼Œé˜²æ­¢è·‘é¢˜ã€‚

ä¸»é¢˜: {topic}
ä¸Šä¸€ç« : {last_chapter_title}
ä¸Šä¸€ç« å†…å®¹ï¼ˆèŠ‚é€‰ï¼‰:
{last_text[:1200]}

å…¨ä¹¦ä¸»æ—¨ï¼ˆå‚è€ƒï¼‰:
{global_thesis[:400]}

æŒ‡ä»¤: è¯·ç»“åˆå…¨ä¹¦æ ¸å¿ƒç›®æ ‡ï¼Œæ€»ç»“ä¸Šä¸€ç« çš„å…³é”®å‘ç°ï¼Œå¹¶ä¸ºä¸‹ä¸€ç« å±•å¼€åšé“ºå«ã€‚ç›´æ¥è¾“å‡ºä¸¤æ®µæ­£æ–‡ï¼Œä¸è¦æ ‡é¢˜ã€‚
"""
    summary = call_pro(prompt, temperature=0.35)
    return summary or global_thesis or ""

def write_checkpoint(checkpoint_path, chapter_index, chapter_title, executive_summary, global_thesis):
    """ä¿å­˜æ–­ç‚¹ï¼šç« èŠ‚è¿›åº¦ + æ‘˜è¦"""
    payload = {
        "last_completed_chapter_index": chapter_index,
        "last_completed_chapter_title": chapter_title,
        "executive_summary": executive_summary,
        "global_thesis": global_thesis,
        "timestamp": datetime.now().isoformat()
    }
    try:
        with open(checkpoint_path, "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ æ–­ç‚¹ä¿å­˜å¤±è´¥: {e}")

def generate_style_guide(topic):
    """[Master Planner - Pro] è¾“å‡ºé£æ ¼æŒ‡å—å¹¶ç¼“å­˜"""
    prompt = f"""
ä½ æ˜¯ [Master Planner - Pro]ï¼Œä¸ºä¸»é¢˜ã€Š{topic}ã€‹åˆ¶å®šé£æ ¼æŒ‡å—ã€‚
è¾“å‡º Markdown è¦ç‚¹ï¼Œè¦†ç›–ï¼š
- è¯­è°ƒ: ä¸“ä¸šã€ç´§å‡‘ã€é¿å…è¥é”€è…”
- ç»“æ„: æ•°æ®å…ˆè¡Œã€é€»è¾‘é€’è¿›ã€ç»“è®ºæ¸…æ™°
- æ•°æ®: ä¿ç•™æ¥æºã€ç¦æ­¢è™šæ„ã€è¡¨æ ¼/å›¾è¡¨å„ 1
- è¯­è¨€: é¿å…é‡å¤ï¼Œå‡å°‘å½¢å®¹è¯ï¼Œä½¿ç”¨è¡Œä¸šæœ¯è¯­
- å›¾è¡¨: åªä¿ç•™ 1 å¼ æ ¸å¿ƒå›¾ï¼Œæè¿°éœ€ç®€æ´
è¯·æ§åˆ¶åœ¨ 200-300 å­—ï¼Œç›´æ¥è¾“å‡º Markdown åˆ—è¡¨ã€‚
"""
    return call_pro(prompt, temperature=0.35)

def plan_section_flash(topic, chapter_title, section_title, subsections, style_guide, executive_summary="", key_constraints=""):
    """[Section Planner - Pro] æ‹†è§£ç« èŠ‚ä»»åŠ¡ï¼ˆä¼˜å…ˆè´¨é‡ï¼‰"""
    subsections_text = ", ".join(subsections[:6])
    prompt = f"""
[Section Planner - Pro]
ä¸»é¢˜: {topic}
ç« èŠ‚: {chapter_title} / {section_title}
å°èŠ‚: {subsections_text}
é£æ ¼æŒ‡å—æ‘˜å½•: {style_guide[:600]}
Executive Summary: {executive_summary[:800]}
Key Constraints:
{key_constraints}

ä»»åŠ¡: æ‹†è§£æœ¬ç« èŠ‚å†™ä½œï¼Œç»™å‡ºï¼š
- æ•°æ®è¦ç‚¹ï¼ˆ3-5 æ¡ï¼Œå«æŒ‡æ ‡/æ—¶é—´èŒƒå›´ï¼‰
- å»ºè®®æœç´¢è¯ï¼ˆ4-6 ä¸ªä¸­è‹±æ–‡æ··åˆï¼‰
- å»ºè®®å›¾è¡¨ï¼ˆç±»å‹ + ç»´åº¦ + æœŸæœ›æ•°æ®åˆ—ï¼‰

è¦æ±‚: Markdown åˆ—è¡¨ï¼Œç®€çŸ­å¯æ‰§è¡Œï¼Œä¸è¦ç©ºè¯ã€‚
"""
    return call_pro(prompt, temperature=0.35)

def writer_flash_draft(topic, chapter_title, section_title, subsection_title, local_ctx, web_ctx, related_ctx, section_plan, style_guide):
    """[Writer - Flash] ç²—å†™å¹²å·´å·´åˆç¨¿"""
    prompt = f"""
[Writer - Flash] è§’è‰²ï¼šè“é¢†é‡‡ç¼–ï¼Œè´Ÿè´£æœç´¢/é˜…è¯»/å¡«å……ï¼Œè¾“å‡ºå¹²å·´å·´ä½†å‡†ç¡®çš„åˆç¨¿ã€‚
ä¸»é¢˜: {topic}
ä½ç½®: {chapter_title} / {section_title} / {subsection_title}
ç« èŠ‚æ‹†è§£: {section_plan}
é£æ ¼æŒ‡å—: {style_guide[:800]}

[æœ¬åœ°èµ„æ–™]
{local_ctx if local_ctx else "ï¼ˆæ— ï¼‰"}

[ç½‘ç»œèµ„æ–™]
{web_ctx if web_ctx else "ï¼ˆæ— ï¼‰"}

[ç›¸å…³ä¸Šä¸‹æ–‡]
{related_ctx if related_ctx else "ï¼ˆæ— ï¼‰"}

å†™ä½œè¦æ±‚:
1) ä¸¥ç¦è™šæ„æ•°æ®ï¼Œä»…å¼•ç”¨èµ„æ–™ä¸­çš„æ•°å­—/ç»“è®ºã€‚
2) ç»“æ„: å…³é”®æ•°æ®è¦ç‚¹ -> åˆ†æè§£é‡Š -> é£é™©/é™åˆ¶ -> å°ç»“ã€‚
3) Markdownï¼Œä½¿ç”¨ 2-3 çº§æ ‡é¢˜ï¼Œè‡³å°‘ 1 ä¸ªè¡¨æ ¼åˆ—å‡ºæ ¸å¿ƒæ•°å­—ã€‚
4) å¼•ç”¨è§„èŒƒï¼šæœ¬åœ°/ç½‘ç»œ/ç›¸å…³ä¸Šä¸‹æ–‡ä¸­å«æœ‰ [æ¥æºè·¯å¾„] é¢åŒ…å±‘ï¼ˆfilename > H1 > H2 > H3ï¼‰ï¼Œå¼•ç”¨å…³é”®æ•°æ®æ—¶åœ¨æ­£æ–‡ç‚¹åæ¥æºï¼›å¯é€‰ä½¿ç”¨ Markdown è„šæ³¨ [^1]ï¼Œè„šæ³¨å†…å®¹å†™â€œæ¥æºè·¯å¾„â€ã€‚
5) å¦‚æœæ•°æ®æ—¶é—´è¾ƒæ—§ï¼ˆä¾‹å¦‚æ¥æºä¸­å¹´ä»½æ—©äºéœ€æ±‚ï¼‰ï¼Œè¯·åœ¨æ­£æ–‡æç¤ºâ€œæ•°æ®æ—¶æ•ˆæ€§â€ã€‚
6) æ–‡é£å¹²ç»ƒã€ç›´æ¥ï¼Œä¸åšåä¸½æªè¾ï¼Œä¿æŒâ€œå·¥ç¨‹æ±‡æŠ¥â€å£å»ã€‚
7) ä¸è¦è¾“å‡º JSON/ä»£ç /å›¾è¡¨å—ï¼Œçº¯æ–‡å­— + è¡¨æ ¼å³å¯ã€‚
"""
    return call_flash(prompt, temperature=0.55, task_type="heavy_reading")

def writer_flash_chart_data(topic, subsection_title, local_ctx, web_ctx, flash_draft, section_plan, style_guide):
    """[Writer - Flash] æç‚¼ç»˜å›¾æ•°æ®"""
    prompt = f"""
[Writer - Flash] åªè´Ÿè´£æç‚¼ç»˜å›¾æ•°æ®ï¼Œä¸å†™ä»£ç ã€‚
ä¸»é¢˜: {topic}
å­èŠ‚: {subsection_title}
ç« èŠ‚æ‹†è§£: {section_plan}
é£æ ¼æŒ‡å—: {style_guide[:400]}

[å¯ç”¨èµ„æ–™]
æœ¬åœ°: {local_ctx if local_ctx else "ï¼ˆæ— ï¼‰"}
ç½‘ç»œ: {web_ctx if web_ctx else "ï¼ˆæ— ï¼‰"}
åˆç¨¿: {flash_draft if flash_draft else "ï¼ˆåˆç¨¿ä¸ºç©ºï¼‰"}

è¯·ç»™å‡ºå•ä¸€ JSON å¯¹è±¡ï¼Œå­—æ®µ: chart_type, title, x_label, y_label, data.labels[], data.datasets[].label/values, sourceã€‚
è§„åˆ™: 
- ä»…ä½¿ç”¨å·²å‡ºç°çš„æ•°æ®ï¼Œä¸è¦çŒœæµ‹ï¼›è‹¥æ•°æ®ä¸è¶³åˆ™è¿”å›ç©ºå¯¹è±¡ {{}}ã€‚
- ä¿è¯ labels å’Œ values æ•°é‡ä¸€è‡´ï¼Œvalues ä¸ºæ•°å­—ã€‚
- é™åˆ¶ 1 ä¸ªå›¾è¡¨ã€‚
"""
    raw = call_flash_json(prompt, temperature=0.25, task_type="heavy_reading")
    chart_data = extract_first_json_block(raw)
    if not chart_data:
        return None
    ok, errs = validate_json_chart_data(chart_data)
    if not ok:
        print(f"      âš ï¸ Flash å›¾è¡¨æ•°æ®æ ¡éªŒå¤±è´¥: {errs}")
        return None
    return chart_data

def writer_pro_chart_data(topic, subsection_title, local_ctx, web_ctx, flash_draft, section_plan, style_guide):
    """[Writer - Pro] å…œåº•æç‚¼ç»˜å›¾æ•°æ®ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰"""
    prompt = f"""
[Writer - Pro] ä»ç°æœ‰èµ„æ–™ä¸­æç‚¼ä¸€ä»½å›¾è¡¨ JSONã€‚
ä¸»é¢˜: {topic}
å­èŠ‚: {subsection_title}
ç« èŠ‚æ‹†è§£: {section_plan}
é£æ ¼æŒ‡å—: {style_guide[:400]}

[å¯ç”¨èµ„æ–™]
æœ¬åœ°: {local_ctx if local_ctx else "ï¼ˆæ— ï¼‰"}
ç½‘ç»œ: {web_ctx if web_ctx else "ï¼ˆæ— ï¼‰"}
åˆç¨¿: {flash_draft if flash_draft else "ï¼ˆåˆç¨¿ä¸ºç©ºï¼‰"}

è¾“å‡º: å•ä¸€ JSON å¯¹è±¡ï¼Œå­—æ®µ: chart_type, title, x_label, y_label, data.labels[], data.datasets[].label/values, sourceã€‚
è§„åˆ™: 
- ä»…ä½¿ç”¨å·²å‡ºç°çš„æ•°æ®ï¼Œä¸è¦çŒœæµ‹ï¼›ä¸è¶³åˆ™è¿”å›ç©ºå¯¹è±¡ {{}}ã€‚
- labels ä¸ values æ•°é‡ä¸€è‡´ï¼Œvalues ä¸ºæ•°å­—ã€‚
- é™åˆ¶ 1 ä¸ªå›¾è¡¨ã€‚
"""
    raw = call_model(prompt, get_model("deep_thinking"), temperature=0.25, response_mime_type="application/json")
    chart_data = extract_first_json_block(raw)
    if not chart_data:
        return None
    ok, errs = validate_json_chart_data(chart_data)
    if not ok:
        print(f"      âš ï¸ Pro å›¾è¡¨æ•°æ®æ ¡éªŒå¤±è´¥: {errs}")
        return None
    return chart_data

def execute_pro_chart_code(code_block, chart_data, output_path):
    """æ‰§è¡Œ Pro ç”Ÿæˆçš„ç»˜å›¾ä»£ç ï¼Œé™åˆ¶å¯ç”¨å…¨å±€å˜é‡"""
    safe_globals = {
        "__builtins__": {
            "abs": abs, "min": min, "max": max, "range": range, "len": len,
            "float": float, "int": int, "sum": sum, "enumerate": enumerate, "round": round, "zip": zip,
            "list": list, "dict": dict, "str": str, "tuple": tuple
        },
        "plt": plt,
        "json": json,
        "math": __import__('math')  # æä¾›å†…ç½® math åº“
    }
    try:
        import numpy as np
        safe_globals["np"] = np
    except ImportError:
        # numpy ä¸å¯ç”¨ï¼Œä»£ç ä¸­å¯èƒ½ä¼šå¤±è´¥ï¼Œä½†è‡³å°‘ä¸ä¼šå¯¼è‡´ç¨‹åºå´©æºƒ
        print("      â„¹ï¸ numpy æœªå®‰è£…ï¼Œå¦‚æœç”Ÿæˆçš„ä»£ç éœ€è¦ numpy ä¼šå¤±è´¥")
    
    safe_globals["chart_data"] = chart_data
    safe_globals["output_path"] = output_path
    local_env = {}
    try:
        exec(code_block, safe_globals, local_env)
        render_fn = local_env.get("render") or safe_globals.get("render")
        if callable(render_fn):
            render_fn(chart_data, output_path)
        plt.close('all')
        return os.path.exists(output_path)
    except Exception as e:
        print(f"      âš ï¸ Pro ç»˜å›¾ä»£ç æ‰§è¡Œå¤±è´¥: {e}")
        return False

def writer_pro_chart(chart_data, output_dir, section_title, chapter_title):
    """[Writer - Pro] æ ¹æ®æ•´ç†å¥½çš„æ•°æ®å†™ Python ç»˜å›¾ä»£ç å¹¶æ‰§è¡Œ"""
    ok, errs = validate_json_chart_data(chart_data)
    if not ok:
        print(f"      âš ï¸ å›¾è¡¨æ•°æ®æ— æ•ˆï¼Œè·³è¿‡ç»˜å›¾: {errs}")
        return None, None
    
    safe_title = re.sub(r'[^\w\-]', '_', f"{chapter_title}_{section_title}")[:40]
    charts_dir = os.path.join(output_dir, "charts")
    os.makedirs(charts_dir, exist_ok=True)
    output_path = os.path.join(charts_dir, f"{safe_title}_{int(time.time())}.png")
    
    prompt = f"""
[Writer - Pro] è§’è‰²ï¼šåªè¯»å·²æ•´ç†å¥½çš„æ•°æ®ï¼Œå†™ Python ç»˜å›¾ä»£ç ã€‚
è¾“å…¥ chart_data (JSON)ï¼š{json.dumps(chart_data, ensure_ascii=False)}
è¦æ±‚:
- åªç”¨ matplotlib / numpyï¼Œä¸è®¿é—®ç½‘ç»œ/æ–‡ä»¶ç³»ç»Ÿï¼Œä¸è°ƒç”¨ç³»ç»Ÿå‘½ä»¤ã€‚
- å®šä¹‰ render(chart_data, output_path) å¹¶åœ¨ä»£ç æœ«å°¾è°ƒç”¨å®ƒã€‚
- ä¿å­˜ä¸º PNG åˆ° output_pathï¼Œé£æ ¼å•†åŠ¡ç®€æ´ï¼Œå¯è¯»æ€§é«˜ã€‚
- åªè¾“å‡ºä¸€ä¸ª ```python``` ä»£ç å—ï¼Œå‹¿è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚
"""
    code_resp = call_pro(prompt, temperature=0.2)
    code_block = extract_code_block(code_resp, "python")
    
    success = False
    if code_block:
        success = execute_pro_chart_code(code_block, chart_data, output_path)
    
    if not success:
        print("      â†˜ï¸ ä½¿ç”¨å›é€€ç»˜å›¾æ–¹æ¡ˆ")
        fallback_json = f"```json\n{json.dumps(chart_data, ensure_ascii=False)}\n```"
        success = create_chart_from_description_plotly(chart_data, output_path) or create_chart_from_description(fallback_json, output_path)
    
    return (output_path if success else None), code_block

def editor_pro_upgrade(topic, chapter_title, section_title, subsection_title, flash_draft, style_guide, section_plan, chart_data, chart_image_path):
    """[Editor - Pro] å‡ç»´æ¶¦è‰²ï¼Œè¾“å‡ºæœ€ç»ˆæˆå“"""
    chart_ref = ""
    if chart_image_path:
        chart_ref = f"![{chart_data.get('title', 'å›¾è¡¨')}]({chart_image_path})"
    prompt = f"""
[Editor - Pro] è§’è‰²ï¼šç™½é¢†å®¡ç¾+æ€è€ƒï¼Œå¯¹ Flash åˆç¨¿åšâ€œå‡ç»´æ‰“å‡»â€ã€‚
ä¸»é¢˜: {topic}
ä½ç½®: {chapter_title} / {section_title} / {subsection_title}
é£æ ¼æŒ‡å—: {style_guide[:800]}
ç« èŠ‚æ‹†è§£: {section_plan}
å›¾è¡¨: {json.dumps(chart_data, ensure_ascii=False) if chart_data else "ï¼ˆæ— å›¾è¡¨ï¼‰"} | å¼•ç”¨: {chart_ref if chart_ref else "æ— "}

[Flash åˆç¨¿]
{flash_draft if flash_draft else "ï¼ˆåˆç¨¿ä¸ºç©ºï¼‰"}

ä»»åŠ¡:
1) ä¿ç•™äº‹å®ä¸æ•°å­—ï¼Œå¢å¼ºé€»è¾‘é€’è¿›å’Œè¡Œä¸šæ´å¯Ÿï¼Œä¿®æ­£è¯­ç—…ã€‚
2) åŠ å…¥è¿‡æ¸¡å¥å’Œç»“è®ºï¼Œçªå‡ºå…³é”®æŒ‡æ ‡ï¼Œé€‚å½“è¡¥å……èƒŒæ™¯ã€‚
3) è‹¥æœ‰å›¾è¡¨ï¼Œæ­£æ–‡ä¸­åµŒå…¥ä¸€æ¬¡ Markdown å¼•ç”¨å¹¶ç»™å‡ºä¸€å¥è§£è¯»ï¼š{chart_ref if chart_ref else "æ— "}ã€‚
4) å¼•ç”¨è§„èŒƒï¼šæ­£æ–‡å¼•ç”¨å…³é”®æ•°æ®æ—¶ç‚¹åâ€œæ¥æºè·¯å¾„â€ï¼ˆæ¥è‡ª Flash ä¸Šä¸‹æ–‡çš„ [æ¥æºè·¯å¾„]: filename > H1 > H2 > H3ï¼‰ï¼Œå¯ä½¿ç”¨ Markdown è„šæ³¨ [^1]ï¼Œè„šæ³¨å†…å®¹å†™æ¥æºè·¯å¾„ã€‚
5) è‹¥å‘ç°æ•°æ®æ—¶æ•ˆæ€§ä¸è¶³ï¼ˆæ—§å¹´ä»½ï¼‰ï¼Œéœ€åœ¨æ–‡ä¸­æé†’â€œæ•°æ®æ—¶æ•ˆæ€§â€ã€‚
6) ç›®æ ‡ 900-1100 å­—ï¼ŒMarkdownï¼Œé¿å…å†å†™ JSON/ä»£ç /å¤§çº²ï¼Œä¸è¦æ·»åŠ é¢å¤– ##/### æ ‡é¢˜ï¼ˆå¤–å±‚ä¼šåŒ…è£¹ï¼‰ã€‚
7) è¯­æ°”: ä¸“ä¸šã€å‡ç»ƒã€å¯å¤ç”¨ã€‚
"""
    return call_pro(prompt, temperature=0.45)

# ================== ğŸš€ ä¸šåŠ¡æµç¨‹ ==================

def main():
    print("==========================================")
    print("   ğŸ­ V23.0 å·¥ç¨‹é‡æ„ç‰ˆç ”æŠ¥å·¥å‚            ")
    print("==========================================")
    
    # 1. é…ç½®è‡ªæ£€
    CONF.validate()
    
    # 2. æ£€æŸ¥ä¾èµ–åº“å¯ç”¨æ€§ï¼ˆå¯é€‰åº“ï¼Œç¼ºå¤±æ—¶ä¸ä¸­æ–­æµç¨‹ï¼‰
    print("\nğŸ“¦ æ£€æŸ¥å¯é€‰ä¾èµ–åº“...")
    
    # æ£€æŸ¥ sklearn
    try:
        import sklearn
        print("âœ… sklearn å·²å®‰è£…ï¼Œå°†å¯ç”¨å‘é‡æ£€ç´¢")
    except ImportError:
        print("â„¹ï¸ sklearn æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å…³é”®è¯åŒ¹é…è¿›è¡Œæ£€ç´¢")
        print("   ğŸ’¡ å¯é€‰ï¼šå®‰è£… sklearn è·å¾—æ›´å¥½çš„æ£€ç´¢æ•ˆæœ")
        print("   pip install scikit-learn")
    
    # æ£€æŸ¥ numpy
    try:
        import numpy
        print("âœ… numpy å·²å®‰è£…ï¼Œæ”¯æŒæ‰€æœ‰å›¾è¡¨ç±»å‹")
    except ImportError:
        print("â„¹ï¸ numpy æœªå®‰è£…ï¼Œéƒ¨åˆ†é«˜çº§å›¾è¡¨ç±»å‹ï¼ˆå¦‚çƒ­åŠ›å›¾ï¼‰å¯èƒ½ä¸å¯ç”¨")
        print("   ğŸ’¡ å¯é€‰ï¼šå®‰è£… numpy è·å¾—å®Œæ•´çš„å›¾è¡¨åŠŸèƒ½")
        print("   pip install numpy")
    
    # æ£€æŸ¥ Plotly å¯ç”¨æ€§
    try:
        import plotly
        print("âœ… Plotly å·²å®‰è£…ï¼Œå°†ç”Ÿæˆäº¤äº’å¼å›¾è¡¨")
    except ImportError:
        print("âš ï¸ Plotly æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ Matplotlib ç”Ÿæˆé™æ€å›¾è¡¨")
        print("   ğŸ’¡ å¯é€‰ï¼šå®‰è£… Plotly è·å¾—æ›´å¥½çš„äº¤äº’ä½“éªŒ")
        print("   pip install plotly kaleido")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨è°ƒæ•´ç»“æ„
    if CONF.TARGET_PAGES > 0:
        print(f"\nğŸ“ ç”¨æˆ·è®¾å®šç›®æ ‡é¡µæ•°: {CONF.TARGET_PAGES} é¡µ")
        chapters, sections, subsections = CONF.calculate_outline_structure()
        print(f"   â””â”€ è‡ªåŠ¨ä¼˜åŒ–ç»“æ„: {chapters} ç«  Ã— {sections} å°èŠ‚ Ã— {subsections} å­èŠ‚")
        print(f"   â””â”€ é¢„è®¡å­—æ•°: {CONF.TARGET_PAGES * CONF.WORDS_PER_PAGE} å­—")
        CONF.OUTLINE_CHAPTERS = chapters
        CONF.OUTLINE_SECTIONS = sections
        CONF.OUTLINE_SUBSECTIONS = subsections
    
    # ä¼°ç®—æœ€ç»ˆé¡µæ•°
    estimated_pages = CONF.estimate_page_count()
    total_subsections = CONF.OUTLINE_CHAPTERS * CONF.OUTLINE_SECTIONS * CONF.OUTLINE_SUBSECTIONS
    print(f"\nğŸ“Š å½“å‰ç”Ÿæˆå‚æ•°é…ç½®:")
    print(f"   â€¢ ç»“æ„: {CONF.OUTLINE_CHAPTERS}ç«  x {CONF.OUTLINE_SECTIONS}èŠ‚ x {CONF.OUTLINE_SUBSECTIONS}å­èŠ‚")
    print(f"   â€¢ æ€»ä»»åŠ¡: {total_subsections} ä¸ªå†™ä½œå•å…ƒ")
    print(f"   â€¢ é¢„ä¼°äº§å‡º: çº¦ {estimated_pages:.1f} é¡µ (æŒ‰ {CONF.WORDS_PER_PAGE}å­—/é¡µ è®¡ç®—)")
    print("â˜ï¸ æœ¬è½®ä½¿ç”¨ Gemini 3.0 Pro Previewï¼ˆå¤§çº² + å†™ä½œï¼‰- æœ€æ–°ç»Ÿä¸€æ¶æ„")

    # 3. è·å–ä¸»é¢˜
    topic = input("\nğŸ‘‰ è¯·è¾“å…¥ç ”æŠ¥ä¸»é¢˜ (ä¾‹å¦‚: ç”µåŠ›ç°è´§å¸‚åœº): ").strip()
    if not topic: return
    
    # 3. è·¯å¾„å‡†å¤‡ - æŒ‰ä¸»é¢˜è‡ªåŠ¨åˆ›å»ºç‹¬ç«‹æ–‡ä»¶å¤¹
    folder_name = topic.replace(" ", "_").replace("/", "_")
    
    # ä¸ºè¯¥ä¸»é¢˜åˆ›å»ºä¸“å±æ–‡ä»¶å¤¹ç»“æ„
    topic_base_dir = os.path.join(CONF.BASE_DIR, folder_name)
    input_dir = os.path.join(topic_base_dir, "materials")  # ç´ æå­˜æ”¾ç›®å½•
    output_dir = os.path.join(topic_base_dir, "output")    # ç”Ÿæˆç»“æœç›®å½•
    search_cache_dir = os.path.join(topic_base_dir, "search_cache")  # æœç´¢ç¼“å­˜ç›®å½•
    
    # è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
    for directory in [input_dir, output_dir, search_cache_dir]:
        if not os.path.exists(directory):
            os.makedirs(directory)
    
    print(f"ğŸ“ ä¸ºä¸»é¢˜ '{topic}' åˆ›å»ºç‹¬ç«‹å·¥ä½œç›®å½•:")
    print(f"   â””â”€ ç´ æç›®å½•: {input_dir}")
    print(f"   â””â”€ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   â””â”€ æœç´¢ç¼“å­˜: {search_cache_dir}")
    
    # 4. åˆå§‹åŒ–çŸ¥è¯†åº“ & ä¸Šä¸‹æ–‡
    kb = MaterialManager(input_dir)
    context_mgr = ContextManager()
    
    # 5. Master Planner - Proï¼šå¤§çº² & é£æ ¼æŒ‡å—ï¼Œå†™å…¥ç¼“å­˜
    print(f"\nğŸ’ [Master Planner - Pro] ç”Ÿæˆå…¨ä¹¦å¤§çº² & é£æ ¼æŒ‡å— -> Context Cache")
    outline_path = os.path.join(output_dir, "Structure.json")
    style_path = os.path.join(output_dir, "Style_Guide.md")
    cache_path = os.path.join(output_dir, "ContextCache.json")
    checkpoint_path = os.path.join(output_dir, "Checkpoint.json")
    
    if os.path.exists(outline_path):
        print("ğŸ“‹ åŠ è½½ç°æœ‰å¤§çº²...")
        with open(outline_path, 'r') as f: outline = json.load(f)
    else:
        prompt = f"""
        Role: Chief Industry Expert in {topic}.
        Task: Design a rigorous, professional research report outline.
        Target Audience: High-level executives and technical directors.
        
        Requirements:
        1. Logical Flow: The structure must follow a logical progression (e.g., Market Analysis -> Technical Architecture -> Implementation Strategy -> Risk Control).
        2. Depth: Ensure deep vertical coverage of specific technologies/policies, not just surface-level breadth.
        3. Structure: {CONF.OUTLINE_CHAPTERS} Chapters -> {CONF.OUTLINE_SECTIONS} Sections -> {CONF.OUTLINE_SUBSECTIONS} Subsections.
        
        Output JSON: {{ "title": "...", "chapters": [ {{ "title": "...", "sections": [ {{ "title": "...", "subsections": ["..."] }} ] }} ] }}
        """
        res = call_gemini(prompt, json_mode=True)
        if not res:
            print("âŒ å¤§çº²ç”Ÿæˆå¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return
        try:
            clean_res = res.replace("```json", "").replace("```", "").strip()
            try:
                outline = json.loads(clean_res)
            except json.JSONDecodeError:
                match = re.search(r'(\{[\s\S]*\})', res)
                if match:
                    clean_json = match.group(1)
                    outline = json.loads(clean_json)
                else:
                    raise ValueError("æ— æ³•æå– JSON å¯¹è±¡")
            with open(outline_path, "w") as f: json.dump(outline, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ å¤§çº² JSON è§£æå¤±è´¥: {e}")
            print(f"ğŸ” åŸå§‹å“åº”ç‰‡æ®µ: {res[:500]}...")
            return
    if os.path.exists(style_path):
        with open(style_path, "r", encoding="utf-8") as f:
            style_guide = f.read()
        print("ğŸ¨ å·²åŠ è½½é£æ ¼æŒ‡å—")
    else:
        style_guide = generate_style_guide(topic) or ""
        with open(style_path, "w", encoding="utf-8") as f:
            f.write(style_guide)
        print("ğŸ¨ æ–°ç”Ÿæˆé£æ ¼æŒ‡å—")
    global_thesis = context_mgr.get_global_thesis()
    if not global_thesis:
        global_thesis = generate_global_thesis(topic, outline)
        context_mgr.set_global_thesis(global_thesis)
    context_mgr.set_master_plan(outline, style_guide)
    try:
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump({
                "outline": outline,
                "style_guide": style_guide,
                "global_thesis": global_thesis,
                "generated_at": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

    # 6. Flash / Pro å†™ä½œæµæ°´çº¿
    print(f"\nğŸ  [Flash x Pro] å¯åŠ¨è“é¢†-ç™½é¢†æµæ°´çº¿...")
    full_book = f"# {outline.get('title', topic)}\n\n"
    failed_sections = []
    any_content = False
    key_constraints = get_key_constraints()
    for i, chap in enumerate(outline.get('chapters', [])):
        chap_title = chap.get('title', f"Chapter {i+1}")
        full_book += f"# {chap_title}\n\n"
        print(f"\nğŸ“– {chap_title}")
        for sec in chap.get('sections', []):
            sec_title = sec.get('title', 'Section')
            full_book += f"## {sec_title}\n\n"
            print(f"   ğŸ“‘ {sec_title}")
            sec_file = os.path.join(output_dir, f"{chap_title[:2]}_{sec_title[:5]}.md".replace(" ", "_").replace("/","-"))
            last_chapter_title = outline.get('chapters', [])[i-1].get('title') if i > 0 else None
            rolling_summary = build_executive_summary(topic, context_mgr, last_chapter_title, global_thesis)
            context_mgr.set_last_exec_summary(rolling_summary)
            section_plan = plan_section_flash(topic, chap_title, sec_title, sec.get('subsections', []), style_guide, rolling_summary, key_constraints)
            if section_plan:
                context_mgr.add_section_plan(f"{chap_title}/{sec_title}", section_plan)
            if os.path.exists(sec_file):
                print("      âœ… å·²å­˜åœ¨ï¼Œè·³è¿‡")
                with open(sec_file, 'r') as f: full_book += f.read() + "\n\n"
                continue
            sec_content = ""
            for sub in sec.get('subsections', []):
                print(f"      âœï¸ [Flash] æ’°å†™: {sub} ...")
                local_ctx = kb.retrieve(f"{topic} {sub}")
                related_ctx = context_mgr.get_related_context(f"{topic} {sub}")
                search_query = f"{topic} {sub} æ•°æ® åˆ†æ ç°çŠ¶"
                web_ctx = search_web(search_query, force=False, cache_dir=search_cache_dir)
                if not web_ctx and (not local_ctx or len(local_ctx) < 500):
                    web_ctx = search_web(f"{topic} {sub}", force=True, cache_dir=search_cache_dir)
                if not local_ctx and not web_ctx:
                    print(f"      âš ï¸ æ— å¤–éƒ¨èµ„æ–™å‘½ä¸­ï¼ˆä¸­è‹±æ–‡éƒ½æ— ï¼‰ï¼Œè·³è¿‡æ­¤èŠ‚ç‚¹")
                    sec_content += f"### {sub}\n\n**âš ï¸ æ•°æ®ç¼ºå¤±**\n\næœ¬èŠ‚ç‚¹ç¼ºå°‘ç›¸å…³çš„å¤–éƒ¨æ•°æ®æºï¼ˆæœç´¢æ— ç»“æœï¼‰ï¼Œä¸ºé¿å…è™šæ„å†…å®¹ï¼Œæš‚ä¸ç”Ÿæˆã€‚è¯·æ‰‹åŠ¨è¡¥å……èµ„æ–™æˆ–è°ƒæ•´ä¸»é¢˜èŒƒå›´ã€‚\n\n"
                    failed_sections.append(f"{chap_title} / {sec_title} / {sub}")
                    continue
                flash_draft = writer_flash_draft(topic, chap_title, sec_title, sub, local_ctx, web_ctx, related_ctx, section_plan, style_guide)
                chart_data = writer_flash_chart_data(topic, sub, local_ctx, web_ctx, flash_draft, section_plan, style_guide)
                if not chart_data:
                    chart_data = writer_pro_chart_data(topic, sub, local_ctx, web_ctx, flash_draft, section_plan, style_guide)
                chart_path = None
                chart_rel_path = ""
                if chart_data:
                    chart_path, _ = writer_pro_chart(chart_data, output_dir, sub, chap_title)
                if chart_path:
                    chart_rel_path = os.path.relpath(chart_path, output_dir)
                    print(f"      ğŸ–¼ï¸ å›¾è¡¨ç”Ÿæˆå®Œæˆ: {chart_rel_path}")
                final_body = editor_pro_upgrade(topic, chap_title, sec_title, sub, flash_draft, style_guide, section_plan, chart_data, chart_rel_path)
                if not final_body and flash_draft:
                    final_body = flash_draft
                if not final_body:
                    print(f"      âŒ æœ¬å­èŠ‚ç”Ÿæˆå¤±è´¥")
                    failed_sections.append(f"{chap_title} / {sec_title} / {sub}")
                    continue
                if chart_rel_path and chart_rel_path not in final_body:
                    final_body += f"\n\n![{chart_data.get('title', 'å›¾è¡¨')}]({chart_rel_path})\n"
                quality_score, feedback, _ = evaluate_content_quality(final_body, topic, sub)
                print(f"      ğŸ“Š è´¨é‡è¯„åˆ†: {quality_score:.1f}/10 | {feedback[:60]}")
                if quality_score < CONF.QUALITY_THRESHOLD:
                    print("      ğŸ” è´¨é‡æœªè¾¾æ ‡ï¼Œåˆ‡æ¢ Pro å†æ¶¦è‰²ä¸€è½®")
                    retry_body = editor_pro_upgrade(topic, chap_title, sec_title, sub, final_body, style_guide, section_plan, chart_data, chart_rel_path)
                    if retry_body:
                        final_body = retry_body
                        quality_score, feedback, _ = evaluate_content_quality(final_body, topic, sub)
                        print(f"      ğŸ“Š äºŒæ¬¡è´¨é‡è¯„åˆ†: {quality_score:.1f}/10 | {feedback[:60]}")
                sec_content += f"### {sub}\n\n{final_body}\n\n"
                any_content = True
                context_mgr.add_section(chap_title, sub, final_body)
            if sec_content:
                with open(sec_file, "w", encoding="utf-8") as f: f.write(sec_content)
                full_book += sec_content
            else:
                print("      âŒ æœ¬èŠ‚æœªç”Ÿæˆæœ‰æ•ˆå†…å®¹")
        # ç« èŠ‚å®Œæˆåä¿å­˜æ–­ç‚¹ï¼ˆä¾¿äºç»­è·‘ï¼‰
        write_checkpoint(checkpoint_path, i, chap_title, context_mgr.get_last_exec_summary(), global_thesis)
    # 8. è£…è®¢
    if not any_content:
        print("\nâŒ æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆç« èŠ‚ï¼Œå·²åœæ­¢è£…è®¢ã€‚è¯·æ£€æŸ¥æœ¬åœ°æ¨¡å‹æˆ–èµ„æ–™ã€‚")
        if failed_sections:
            print("æœªå®Œæˆåˆ—è¡¨:")
            for item in failed_sections: print(f" - {item}")
        return

    final_path = os.path.join(output_dir, "Final_Book.md")
    with open(final_path, "w", encoding="utf-8") as f: f.write(full_book)
    print(f"\nğŸ‰ğŸ‰ğŸ‰ ä»»åŠ¡å®Œæˆï¼æ–‡ä»¶: {final_path}")
    
    # æ‰“å°ç”Ÿæˆç»Ÿè®¡
    print(f"\nğŸ“Š æœ€ç»ˆç”Ÿæˆç»Ÿè®¡:")
    print(f"   å·²ç”Ÿæˆç« èŠ‚æ•°: {len(context_mgr.generated_sections)}")
    
    total_sections_gen = sum(len(secs) for secs in context_mgr.generated_sections.values())
    
    # è®¡ç®—å®é™…å­—æ•°
    total_chars = 0
    for ch_content in context_mgr.generated_sections.values():
        for sec_content in ch_content.values():
            total_chars += len(sec_content)
            
    actual_pages = total_chars / CONF.WORDS_PER_PAGE
    
    print(f"   å·²ç”Ÿæˆå°èŠ‚æ•°: {total_sections_gen}")
    print(f"   æ€»å­—æ•°: {total_chars:,} å­—")
    print(f"   å®é™…é¡µæ•°: {actual_pages:.1f} é¡µ (ç›®æ ‡: {CONF.TARGET_PAGES} é¡µ)")
    print(f"   å¤±è´¥å°èŠ‚æ•°: {len(failed_sections)}")
    
    print(f"\n{context_mgr.get_summary()}")
    
    if failed_sections:
        print("âš ï¸ ä»¥ä¸‹å­èŠ‚å†™ä½œå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¡¥å†™æˆ–é‡è·‘:")
        for item in failed_sections: print(f" - {item}")

    # è‡ªåŠ¨è½¬æ¢ Wordï¼ˆå¸¦æ ·å¼ï¼‰
    docx_name = f"{folder_name}_Research_Report.docx"
    docx_path = os.path.join(output_dir, docx_name)
    ref_doc = get_reference_doc_path()
    if ref_doc and not os.path.exists(ref_doc):
        print(f"âš ï¸ æœªæ‰¾åˆ°æ ·å¼æ¯ç‰ˆ {ref_doc}ï¼Œå°†ä½¿ç”¨ Pandoc é»˜è®¤æ ·å¼ã€‚")
        ref_doc = None
    convert_md_to_docx(
        md_filename=final_path,
        output_filename=docx_path,
        reference_doc=ref_doc,
        resource_path=output_dir
    )
    print(f"ğŸ“„ å·²ç”Ÿæˆ Word æ–‡æ¡£: {docx_path}")

# ============ Word è½¬æ¢å·¥å…· ============ #

def get_reference_doc_path():
    """è¿”å›æ ·å¼æ¯ç‰ˆè·¯å¾„ï¼Œé»˜è®¤æ”¾åœ¨ BASE_DIR ä¸‹"""
    return os.path.join(CONF.BASE_DIR, "reference.docx")

def convert_md_to_docx(md_filename, output_filename, reference_doc=None, resource_path="."):
    """
    å°† Markdown è½¬æ¢ä¸ºæ’ç‰ˆå®Œç¾çš„ Wordï¼Œæ”¯æŒæ ·å¼æ¯ç‰ˆä¸èµ„æºè·¯å¾„ã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨å°† {md_filename} è½¬æ¢ä¸º Word æ–‡æ¡£...")
    extra_args = [
        '--toc',
        '--toc-depth=3',
        f'--resource-path={resource_path}',
        '--standalone'
    ]
    if reference_doc and os.path.exists(reference_doc):
        extra_args.append(f'--reference-doc={reference_doc}')
    try:
        import pypandoc
        pypandoc.convert_file(
            md_filename,
            'docx',
            outputfile=output_filename,
            extra_args=extra_args
        )
        print(f"âœ… è½¬æ¢æˆåŠŸï¼æ–‡æ¡£å·²ç”Ÿæˆ: {output_filename}")
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        print(f"å»ºè®®æ‰‹åŠ¨æ‰§è¡Œå‘½ä»¤: pandoc {md_filename} -o {output_filename} {'--reference-doc='+reference_doc if reference_doc else ''} --toc")

if __name__ == "__main__":
    main()
